{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Word_Vectors_2021_group13.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glA8bwEfX4Ty"
      },
      "source": [
        "# Assignment 2A Group 13\n",
        "\n",
        "* Rivka Vollebregt, 8842507\n",
        "* Jacob Hemming, 6433383\n",
        "* Jorrit Jan Walinga, 6498027\n",
        "* Sunny Hsieh, 6534856\n",
        "\n",
        "a2ee308b-db84-4aba-95da-fd2e77c2ad72\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZitizQuFsUDc"
      },
      "source": [
        "# Vectorial Word Representations\n",
        "\n",
        "## Background\n",
        "Representing words as dense vectors over a finite-dimensional space was one of the recent breakthroughs in Natural Language Processing. Vectorial representations allow space-efficient, informationally rich storage of words that adequately captures their semantic content and enables numerical computation on them. Word vectors are the standard input representation for machine learning architectures for language processing. Even though new methods for constructing such representations emerge frequently, the original set of published papers remain a de facto point of reference as well as a good starting point. For this assignment, you will be asked to implement a small-scale variant of one such paper, namely [Global Word Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf) (\"the GloVe paper\").\n",
        "\n",
        "Much of the code and data pre-processing has already been done for you. Additionally, notes on the paper will appear throughout the notebook to guide you along the code. It is, however, important to read and understand the paper, its terminology and the theory behind it before attempting to go through with the assignment. Some of the tasks will also require addressing the paper directly.\n",
        "\n",
        "**-------------------------------------------------------------------------------------------------------------**\n",
        "\n",
        "There are 2 types of tasks in this assignment: \n",
        "- coding tasks --- 10 tasks worth 1 point each --- asking you to write code following specifications provided; make sure to <ins>show the code to your teacher after completing every coding task</ins>\n",
        "- interpretation questions --- 5 questions worth 1 point each --- asking you to interpret the data or the results of the model\n",
        "\n",
        "You are greatly encouraged to add comments to your code describing what particular lines of code do (in general, a great habit to have in your coding life), as well as self-check regularly by printing your tensors and their shapes making sure they look adequate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhDYxhnBsUDf"
      },
      "source": [
        "## Corpus Statistics\n",
        "\n",
        "The paper's proposed model, GloVe, aims to densely represent words in a way that captures the global corpus statistics. \n",
        "\n",
        "The construction it encodes is the word __co-occurrence matrix__. A co-occurrence matrix is a simplistic data structure that counts the number of times each word has appeared within the context of every other word. The definition of a context varies; usually, context is implied to be a fixed-length span (that may or may not be allowed to escape sentence boundaries) around a word. \n",
        "\n",
        "For instance, in the sentence below and for a context length of 2, the word <span style=\"color:pink\">__Earth__</span> occurs in the context of <span style=\"color:lightgreen\">made</span> (1), <span style=\"color:lightgreen\">on</span> (1), <span style=\"color:lightgreen\">as</span> (1), <span style=\"color:lightgreen\">an</span> (1).\n",
        "\n",
        "> \"He struck most of the friends he had <span style=\"color:lightgreen\">made on</span> <span style=\"color:pink\">__Earth__</span> <span style=\"color:lightgreen\">as an</span> eccentric\"\n",
        "\n",
        "Similarly, the word <span style=\"color:pink\">__friends__</span> occurs in the context of <span style=\"color:lightgreen\">of</span> (1), <span style=\"color:lightgreen\">the</span> (1), <span style=\"color:lightgreen\">he</span> (1), <span style=\"color:lightgreen\">had</span> (1).\n",
        "\n",
        "> \"He struck most <span style=\"color:lightgreen\">of the</span> <span style=\"color:pink\">__friends__</span> <span style=\"color:lightgreen\">he had</span> made on Earth as an eccentric\"\n",
        "\n",
        "An alternative definition of a context would be, for instance, the variable-length windows spanned by a full sentence.\n",
        "\n",
        "Contexts may be summed across sentences or entire corpora; the summed context of <span style=\"color:pink\">he</span> in the example sentence is: <span style=\"color:lightgreen\">struck</span> (1), <span style=\"color:lightgreen\">most</span> (1), <span style=\"color:lightgreen\">the</span> (1), <span style=\"color:lightgreen\">friends</span> (1), <span style=\"color:lightgreen\">had</span> (1), <span style=\"color:lightgreen\">made</span> (1).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imo-I4lesUDg"
      },
      "source": [
        "For the purposes of this assignment, we have prepared a co-occurrence matrix over a minimally processed version of the Harry Potter books.\n",
        "\n",
        "(A few interpretation tasks in this assignment presuppose some minimal level of familiarity with the Harry Potter books/films. If no one in your group is familiar with Harry Potter, please talk to your teacher)\n",
        "\n",
        "The pickle file contains three items:\n",
        "1. `vocab`: a dictionary mapping words to unique ids, containing $N$ unique words\n",
        "2. `contexts`: a dictionary mapping words to their contexts, where contexts are themselves dicts from words to integers that show the number of co-occurrences between these words.\n",
        "    E.g. `{\"portrait\": {\"harry\": 103, \"said\": 97, ...}, ...}` meaning that the word \"harry\" has appeared in the context of the word \"portrait\" 103 times, etc.\n",
        "3. `X`: a torch LongTensor ${X}$ of size $N \\times N$, where ${X}[i,j]$ denotes the number of times the word with id $j$ has appeared in the context of the word with id $i$\n",
        "\n",
        "Extremely common or uncommon words (i.e. words with too few or too many global occurrences) have been filtered out for practical reasons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5bWqwknsUDh"
      },
      "source": [
        "import pickle\n",
        "import torch\n",
        "from torch import FloatTensor, LongTensor\n",
        "from typing import Dict, Callable, List"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81678ljWiAIO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d28d0bae-19af-4140-f767-e48776dd795a"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4M0MQrNvsUDi",
        "outputId": "edad4709-d760-4989-ec4b-cffd30f58369"
      },
      "source": [
        "#with open(\"/content/drive/MyDrive/Utrecht/Artificial Intelligence/MLHVL/output.p\", \"rb\") as f:\n",
        "#    vocab, contexts, X = pickle.load(f)\n",
        "#print(len(vocab))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5359\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"output.p\", \"rb\") as f:\n",
        "    vocab, contexts, X = pickle.load(f)\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "id": "PAjO892-e0f7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frnz1c1asUDi"
      },
      "source": [
        "Let's inspect the summed context of the word 'portrait'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-f3qC3ZsUDj",
        "outputId": "7e5d1ea8-c68f-4770-f870-586bf677c044"
      },
      "source": [
        "sorted([(item, value) for item, value in contexts[\"portrait\"].items()], key=lambda x: x[1], reverse=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('harry', 103),\n",
              " ('said', 97),\n",
              " ('hole', 84),\n",
              " ('ron', 49),\n",
              " ('hermione', 46),\n",
              " ('room', 41),\n",
              " ('t', 40),\n",
              " ('fat', 39),\n",
              " ('lady', 39),\n",
              " ('common', 30),\n",
              " ('dumbledore', 26),\n",
              " ('phineas', 24),\n",
              " ('climbed', 22),\n",
              " ('just', 19),\n",
              " ('swung', 17),\n",
              " ('sirius', 17),\n",
              " ('professor', 17),\n",
              " ('time', 16),\n",
              " ('voice', 15),\n",
              " ('open', 15),\n",
              " ('got', 15),\n",
              " ('nigellus', 15),\n",
              " ('reached', 14),\n",
              " ('like', 14),\n",
              " ('came', 14),\n",
              " ('little', 14),\n",
              " ('gryffindor', 13),\n",
              " ('turned', 13),\n",
              " ('forward', 12),\n",
              " ('don', 12),\n",
              " ('long', 12),\n",
              " ('place', 12),\n",
              " ('wall', 11),\n",
              " ('neville', 11),\n",
              " ('black', 11),\n",
              " ('going', 11),\n",
              " ('snape', 11),\n",
              " ('hall', 11),\n",
              " ('mcgonagall', 11),\n",
              " ('corridor', 10),\n",
              " ('walked', 10),\n",
              " ('away', 10),\n",
              " ('ve', 10),\n",
              " ('way', 10),\n",
              " ('visit', 10),\n",
              " ('good', 10),\n",
              " ('did', 10),\n",
              " ('look', 10),\n",
              " ('password', 9),\n",
              " ('moment', 9),\n",
              " ('know', 9),\n",
              " ('sir', 9),\n",
              " ('opened', 9),\n",
              " ('face', 9),\n",
              " ('heard', 9),\n",
              " ('come', 8),\n",
              " ('gone', 8),\n",
              " ('asked', 8),\n",
              " ('let', 8),\n",
              " ('really', 8),\n",
              " ('entrance', 8),\n",
              " ('cadogan', 8),\n",
              " ('mother', 8),\n",
              " ('door', 8),\n",
              " ('pushed', 7),\n",
              " ('inside', 7),\n",
              " ('cloak', 7),\n",
              " ('looked', 7),\n",
              " ('picture', 7),\n",
              " ('staircase', 7),\n",
              " ('oh', 7),\n",
              " ('tower', 7),\n",
              " ('head', 7),\n",
              " ('ginny', 7),\n",
              " ('thought', 7),\n",
              " ('passed', 7),\n",
              " ('minister', 7),\n",
              " ('fudge', 7),\n",
              " ('scrambled', 6),\n",
              " ('looking', 6),\n",
              " ('bed', 6),\n",
              " ('hagrid', 6),\n",
              " ('himself', 6),\n",
              " ('point', 6),\n",
              " ('house', 6),\n",
              " ('castle', 6),\n",
              " ('tried', 6),\n",
              " ('coming', 6),\n",
              " ('need', 6),\n",
              " ('hurried', 6),\n",
              " ('stood', 6),\n",
              " ('saw', 6),\n",
              " ('people', 6),\n",
              " ('knew', 6),\n",
              " ('frame', 6),\n",
              " ('eyes', 6),\n",
              " ('dean', 6),\n",
              " ('wizard', 6),\n",
              " ('grimmauld', 6),\n",
              " ('headmaster', 6),\n",
              " ('won', 5),\n",
              " ('dress', 5),\n",
              " ('followed', 5),\n",
              " ('running', 5),\n",
              " ('seventh', 5),\n",
              " ('floor', 5),\n",
              " ('thing', 5),\n",
              " ('dormitory', 5),\n",
              " ('stairs', 5),\n",
              " ('pulled', 5),\n",
              " ('went', 5),\n",
              " ('m', 5),\n",
              " ('waiting', 5),\n",
              " ('yelled', 5),\n",
              " ('colin', 5),\n",
              " ('took', 5),\n",
              " ('left', 5),\n",
              " ('gryffindors', 5),\n",
              " ('talking', 5),\n",
              " ('doors', 5),\n",
              " ('felt', 5),\n",
              " ('office', 5),\n",
              " ('fred', 5),\n",
              " ('great', 5),\n",
              " ('set', 5),\n",
              " ('gave', 5),\n",
              " ('approached', 5),\n",
              " ('downstairs', 5),\n",
              " ('parchment', 5),\n",
              " ('dinner', 5),\n",
              " ('silver', 5),\n",
              " ('walk', 5),\n",
              " ('prime', 5),\n",
              " ('end', 4),\n",
              " ('hung', 4),\n",
              " ('woman', 4),\n",
              " ('pink', 4),\n",
              " ('percy', 4),\n",
              " ('reveal', 4),\n",
              " ('chair', 4),\n",
              " ('couldn', 4),\n",
              " ('care', 4),\n",
              " ('didn', 4),\n",
              " ('herself', 4),\n",
              " ('painting', 4),\n",
              " ('mind', 4),\n",
              " ('entered', 4),\n",
              " ('crept', 4),\n",
              " ('appeared', 4),\n",
              " ('stand', 4),\n",
              " ('short', 4),\n",
              " ('crowd', 4),\n",
              " ('spiral', 4),\n",
              " ('deserted', 4),\n",
              " ('caught', 4),\n",
              " ('began', 4),\n",
              " ('waited', 4),\n",
              " ('climbing', 4),\n",
              " ('large', 4),\n",
              " ('called', 4),\n",
              " ('boys', 4),\n",
              " ('past', 4),\n",
              " ('headed', 4),\n",
              " ('nearly', 4),\n",
              " ('years', 4),\n",
              " ('minutes', 4),\n",
              " ('closed', 4),\n",
              " ('later', 4),\n",
              " ('canvas', 4),\n",
              " ('able', 4),\n",
              " ('taken', 4),\n",
              " ('half', 4),\n",
              " ('simply', 4),\n",
              " ('ask', 4),\n",
              " ('sitting', 4),\n",
              " ('returned', 4),\n",
              " ('george', 4),\n",
              " ('night', 4),\n",
              " ('think', 4),\n",
              " ('lavender', 4),\n",
              " ('tell', 4),\n",
              " ('screaming', 4),\n",
              " ('old', 4),\n",
              " ('doing', 4),\n",
              " ('curtains', 4),\n",
              " ('figures', 4),\n",
              " ('ran', 4),\n",
              " ('odd', 4),\n",
              " ('painted', 4),\n",
              " ('bedroom', 4),\n",
              " ('desk', 4),\n",
              " ('barely', 4),\n",
              " ('silence', 4),\n",
              " ('explanation', 4),\n",
              " ('ugly', 4),\n",
              " ('man', 4),\n",
              " ('bag', 4),\n",
              " ('listen', 3),\n",
              " ('silk', 3),\n",
              " ('needed', 3),\n",
              " ('leg', 3),\n",
              " ('fireplace', 3),\n",
              " ('armchairs', 3),\n",
              " ('spoke', 3),\n",
              " ('believe', 3),\n",
              " ('wanted', 3),\n",
              " ('stop', 3),\n",
              " ('pig', 3),\n",
              " ('snout', 3),\n",
              " ('save', 3),\n",
              " ('packed', 3),\n",
              " ('use', 3),\n",
              " ('stopped', 3),\n",
              " ('managed', 3),\n",
              " ('climb', 3),\n",
              " ('invisibility', 3),\n",
              " ('sorry', 3),\n",
              " ('hurrying', 3),\n",
              " ('fight', 3),\n",
              " ('new', 3),\n",
              " ('impatiently', 3),\n",
              " ('standing', 3),\n",
              " ('leaving', 3),\n",
              " ('creevey', 3),\n",
              " ('lockhart', 3),\n",
              " ('trolls', 3),\n",
              " ('pointing', 3),\n",
              " ('wait', 3),\n",
              " ('gray', 3),\n",
              " ('immediately', 3),\n",
              " ('closing', 3),\n",
              " ('corridors', 3),\n",
              " ('hidden', 3),\n",
              " ('trouble', 3),\n",
              " ('threw', 3),\n",
              " ('right', 3),\n",
              " ('watch', 3),\n",
              " ('d', 3),\n",
              " ('rest', 3),\n",
              " ('students', 3),\n",
              " ('heads', 3),\n",
              " ('vanished', 3),\n",
              " ('crookshanks', 3),\n",
              " ('shut', 3),\n",
              " ('dormitories', 3),\n",
              " ('completely', 3),\n",
              " ('mad', 3),\n",
              " ('shall', 3),\n",
              " ('outside', 3),\n",
              " ('read', 3),\n",
              " ('tiny', 3),\n",
              " ('walls', 3),\n",
              " ('extremely', 3),\n",
              " ('met', 3),\n",
              " ('table', 3),\n",
              " ('grounds', 3),\n",
              " ('corner', 3),\n",
              " ('sight', 3),\n",
              " ('told', 3),\n",
              " ('circular', 3),\n",
              " ('hand', 3),\n",
              " ('silent', 3),\n",
              " ('noise', 3),\n",
              " ('eye', 3),\n",
              " ('admit', 3),\n",
              " ('stay', 3),\n",
              " ('clambered', 3),\n",
              " ('realized', 3),\n",
              " ('mrs', 3),\n",
              " ('taking', 3),\n",
              " ('expression', 3),\n",
              " ('movement', 3),\n",
              " ('hair', 3),\n",
              " ('slightly', 3),\n",
              " ('wearing', 3),\n",
              " ('wand', 3),\n",
              " ('trying', 3),\n",
              " ('dark', 3),\n",
              " ('marching', 3),\n",
              " ('calling', 3),\n",
              " ('introduce', 3),\n",
              " ('announced', 3),\n",
              " ('arrival', 3),\n",
              " ('mentioned', 3),\n",
              " ('romilda', 3),\n",
              " ('vane', 3),\n",
              " ('word', 3),\n",
              " ('sped', 3),\n",
              " ('witches', 3),\n",
              " ('armor', 3),\n",
              " ('steps', 3),\n",
              " ('girl', 3),\n",
              " ('ariana', 3),\n",
              " ('severus', 3),\n",
              " ('bloody', 2),\n",
              " ('control', 2),\n",
              " ('prefects', 2),\n",
              " ('glowing', 2),\n",
              " ('shadows', 2),\n",
              " ('wasn', 2),\n",
              " ('easily', 2),\n",
              " ('angry', 2),\n",
              " ('train', 2),\n",
              " ('tomorrow', 2),\n",
              " ('nighttime', 2),\n",
              " ('shoulders', 2),\n",
              " ('toppled', 2),\n",
              " ('guess', 2),\n",
              " ('stuck', 2),\n",
              " ('clock', 2),\n",
              " ('burst', 2),\n",
              " ('clearly', 2),\n",
              " ('cut', 2),\n",
              " ('arrive', 2),\n",
              " ('arms', 2),\n",
              " ('better', 2),\n",
              " ('camera', 2),\n",
              " ('longbottom', 2),\n",
              " ('held', 2),\n",
              " ('winking', 2),\n",
              " ('shoulder', 2),\n",
              " ('quidditch', 2),\n",
              " ('practice', 2),\n",
              " ('watched', 2),\n",
              " ('board', 2),\n",
              " ('justin', 2),\n",
              " ('usually', 2),\n",
              " ('snow', 2),\n",
              " ('counting', 2),\n",
              " ('distant', 2),\n",
              " ('sounds', 2),\n",
              " ('throwing', 2),\n",
              " ('teachers', 2),\n",
              " ('slid', 2),\n",
              " ('miserable', 2),\n",
              " ('crossed', 2),\n",
              " ('lot', 2),\n",
              " ('marble', 2),\n",
              " ('girls', 2),\n",
              " ('hasn', 2),\n",
              " ('glad', 2),\n",
              " ('weren', 2),\n",
              " ('second', 2),\n",
              " ('work', 2),\n",
              " ('turn', 2),\n",
              " ('feast', 2),\n",
              " ('glancing', 2),\n",
              " ('isn', 2),\n",
              " ('curiously', 2),\n",
              " ('closer', 2),\n",
              " ('torn', 2),\n",
              " ('whisper', 2),\n",
              " ('moving', 2),\n",
              " ('hiding', 2),\n",
              " ('ripped', 2),\n",
              " ('firmly', 2),\n",
              " ('cloaks', 2),\n",
              " ('oak', 2),\n",
              " ('christmas', 2),\n",
              " ('party', 2),\n",
              " ('previous', 2),\n",
              " ('headmasters', 2),\n",
              " ('sat', 2),\n",
              " ('carried', 2),\n",
              " ('staring', 2),\n",
              " ('stared', 2),\n",
              " ('delighted', 2),\n",
              " ('match', 2),\n",
              " ('getting', 2),\n",
              " ('weasley', 2),\n",
              " ('shaking', 2),\n",
              " ('breath', 2),\n",
              " ('em', 2),\n",
              " ('landing', 2),\n",
              " ('laughing', 2),\n",
              " ('bit', 2),\n",
              " ('concealed', 2),\n",
              " ('peeves', 2),\n",
              " ('ears', 2),\n",
              " ('join', 2),\n",
              " ('potter', 2),\n",
              " ('say', 2),\n",
              " ('annoyed', 2),\n",
              " ('laugh', 2),\n",
              " ('year', 2),\n",
              " ('demanded', 2),\n",
              " ('seen', 2),\n",
              " ('friend', 2),\n",
              " ('yell', 2),\n",
              " ('woke', 2),\n",
              " ('shown', 2),\n",
              " ('mood', 2),\n",
              " ('quite', 2),\n",
              " ('life', 2),\n",
              " ('rolling', 2),\n",
              " ('wake', 2),\n",
              " ('led', 2),\n",
              " ('added', 2),\n",
              " ('hear', 2),\n",
              " ('free', 2),\n",
              " ('idea', 2),\n",
              " ('scowled', 2),\n",
              " ('halt', 2),\n",
              " ('correct', 2),\n",
              " ('revealing', 2),\n",
              " ('talk', 2),\n",
              " ('kind', 2),\n",
              " ('light', 2),\n",
              " ('kept', 2),\n",
              " ('allowed', 2),\n",
              " ('sound', 2),\n",
              " ('dead', 2),\n",
              " ('cold', 2),\n",
              " ('covered', 2),\n",
              " ('hastily', 2),\n",
              " ('feet', 2),\n",
              " ('seamus', 2),\n",
              " ('panting', 2),\n",
              " ('view', 2),\n",
              " ('st', 2),\n",
              " ('mungo', 2),\n",
              " ('looks', 2),\n",
              " ('bad', 2),\n",
              " ('clever', 2),\n",
              " ('pointed', 2),\n",
              " ('slytherin', 2),\n",
              " ('longer', 2),\n",
              " ('message', 2),\n",
              " ('traveling', 2),\n",
              " ('study', 2),\n",
              " ('healer', 2),\n",
              " ('sideways', 2),\n",
              " ('queue', 2),\n",
              " ('shining', 2),\n",
              " ('middle', 2),\n",
              " ('amused', 2),\n",
              " ('apparently', 2),\n",
              " ('red', 2),\n",
              " ('forest', 2),\n",
              " ('dawn', 2),\n",
              " ('arguing', 2),\n",
              " ('broken', 2),\n",
              " ('pictures', 2),\n",
              " ('dippet', 2),\n",
              " ('owe', 2),\n",
              " ('hurtled', 2),\n",
              " ('ignoring', 2),\n",
              " ('tonight', 2),\n",
              " ('naturally', 2),\n",
              " ('uncomfortable', 2),\n",
              " ('curly', 2),\n",
              " ('digging', 2),\n",
              " ('ear', 2),\n",
              " ('wish', 2),\n",
              " ('image', 2),\n",
              " ('number', 2),\n",
              " ('boy', 2),\n",
              " ('joined', 2),\n",
              " ('okay', 2),\n",
              " ('feeling', 2),\n",
              " ('darted', 2),\n",
              " ('late', 2),\n",
              " ('make', 2),\n",
              " ('merely', 2),\n",
              " ('sort', 2),\n",
              " ('leapt', 2),\n",
              " ('aside', 2),\n",
              " ('fell', 2),\n",
              " ('cried', 2),\n",
              " ('help', 2),\n",
              " ('hit', 2),\n",
              " ('small', 2),\n",
              " ('group', 2),\n",
              " ('understood', 2),\n",
              " ('chest', 2),\n",
              " ('folded', 2),\n",
              " ('thinking', 2),\n",
              " ('hogwarts:', 2),\n",
              " ('moon', 2),\n",
              " ('spectacles', 2),\n",
              " ('died', 2),\n",
              " ('forgotten', 2),\n",
              " ('bring', 2),\n",
              " ('departure', 2),\n",
              " ('gruffly', 2),\n",
              " ('larger', 2),\n",
              " ('fang', 2),\n",
              " ('galloping', 2),\n",
              " ('alongside', 2),\n",
              " ('wizards', 2),\n",
              " ('mudblood', 2),\n",
              " ('scene', 2),\n",
              " ('tears', 2),\n",
              " ('voldemort', 2),\n",
              " ('granger', 2),\n",
              " ('sword', 2),\n",
              " ('baron', 1),\n",
              " ('round', 1),\n",
              " ('turning', 1),\n",
              " ('hunched', 1),\n",
              " ('nearest', 1),\n",
              " ('lamp', 1),\n",
              " ('hissing', 1),\n",
              " ('remember', 1),\n",
              " ('home', 1),\n",
              " ('facing', 1),\n",
              " ('cared', 1),\n",
              " ('space', 1),\n",
              " ('possible', 1),\n",
              " ('monster', 1),\n",
              " ('earth', 1),\n",
              " ('hanging', 1),\n",
              " ('flushed', 1),\n",
              " ('sweaty', 1),\n",
              " ('faces', 1),\n",
              " ('panted', 1),\n",
              " ('collapsed', 1),\n",
              " ('trembling', 1),\n",
              " ('saving', 1),\n",
              " ('hadn', 1),\n",
              " ('locked', 1),\n",
              " ('reminded', 1),\n",
              " ('noisy', 1),\n",
              " ('quickly', 1),\n",
              " ('play', 1),\n",
              " ('legs', 1),\n",
              " ('recognized', 1),\n",
              " ('locker', 1),\n",
              " ('curse', 1),\n",
              " ('midnight', 1),\n",
              " ('tail', 1),\n",
              " ('wailed', 1),\n",
              " ('desperate', 1),\n",
              " ('exploded', 1),\n",
              " ('idiot', 1),\n",
              " ('words', 1),\n",
              " ('sudden', 1),\n",
              " ('storm', 1),\n",
              " ('clapping', 1),\n",
              " ('lopsided', 1),\n",
              " ('tables', 1),\n",
              " ('squashy', 1),\n",
              " ('pull', 1),\n",
              " ('brilliant', 1),\n",
              " ('smirking', 1),\n",
              " ('mr', 1),\n",
              " ('beaming', 1),\n",
              " ('double', 1),\n",
              " ('sign', 1),\n",
              " ('fumbled', 1),\n",
              " ('bell', 1),\n",
              " ('rang', 1),\n",
              " ('picked', 1),\n",
              " ('copy', 1),\n",
              " ('gilderoy', 1),\n",
              " ('order', 1),\n",
              " ('merlin', 1),\n",
              " ('class', 1),\n",
              " ('member', 1),\n",
              " ('nimbus', 1),\n",
              " ('thousand', 1),\n",
              " ('clatter', 1),\n",
              " ('dashing', 1),\n",
              " ('swinging', 1),\n",
              " ('hurry', 1),\n",
              " ('wow', 1),\n",
              " ('game', 1),\n",
              " ('knight', 1),\n",
              " ('horse', 1),\n",
              " ('dragged', 1),\n",
              " ('important', 1),\n",
              " ('wondering', 1),\n",
              " ('darker', 1),\n",
              " ('swirling', 1),\n",
              " ('attacks', 1),\n",
              " ('urge', 1),\n",
              " ('thinks', 1),\n",
              " ('somewhat', 1),\n",
              " ('awkwardly', 1),\n",
              " ('ghost', 1),\n",
              " ('ravenclaw', 1),\n",
              " ('seizing', 1),\n",
              " ('difficult', 1),\n",
              " ('journey', 1),\n",
              " ('dodging', 1),\n",
              " ('weasleys', 1),\n",
              " ('darkness', 1),\n",
              " ('falling', 1),\n",
              " ('activity', 1),\n",
              " ('tired', 1),\n",
              " ('sadly', 1),\n",
              " ('remembering', 1),\n",
              " ('divided', 1),\n",
              " ('separate', 1),\n",
              " ('dementors', 1),\n",
              " ('things', 1),\n",
              " ('meet', 1),\n",
              " ('anybody', 1),\n",
              " ('entirely', 1),\n",
              " ('sure', 1),\n",
              " ('october', 1),\n",
              " ('halloween', 1),\n",
              " ('excellent', 1),\n",
              " ('zonko', 1),\n",
              " ('jerking', 1),\n",
              " ('chattering', 1),\n",
              " ('older', 1),\n",
              " ('forehead', 1),\n",
              " ('library', 1),\n",
              " ('choice', 1),\n",
              " ('waking', 1),\n",
              " ('grumpily', 1),\n",
              " ('checked', 1),\n",
              " ('starting', 1),\n",
              " ('discussing', 1),\n",
              " ('dropped', 1),\n",
              " ('nervously', 1),\n",
              " ('usual', 1),\n",
              " ('path', 1),\n",
              " ('ended', 1),\n",
              " ('jammed', 1),\n",
              " ('peered', 1),\n",
              " ('bustling', 1),\n",
              " ('importantly', 1),\n",
              " ('arrived', 1),\n",
              " ('sweeping', 1),\n",
              " ('squeezed', 1),\n",
              " ('moved', 1),\n",
              " ('grabbed', 1),\n",
              " ('arm', 1),\n",
              " ('slashed', 1),\n",
              " ('littered', 1),\n",
              " ('map', 1),\n",
              " ('replaced', 1),\n",
              " ('happy', 1),\n",
              " ('spent', 1),\n",
              " ('sneaking', 1),\n",
              " ('breakfast', 1),\n",
              " ('yellow', 1),\n",
              " ('men', 1),\n",
              " ('enjoying', 1),\n",
              " ('couple', 1),\n",
              " ('handle', 1),\n",
              " ('shiny', 1),\n",
              " ('pointless', 1),\n",
              " ('angle', 1),\n",
              " ('accompanied', 1),\n",
              " ('certain', 1),\n",
              " ('informed', 1),\n",
              " ('heel', 1),\n",
              " ('firebolt', 1),\n",
              " ('tin', 1),\n",
              " ('high', 1),\n",
              " ('finish', 1),\n",
              " ('clutched', 1),\n",
              " ('startled', 1),\n",
              " ('eat', 1),\n",
              " ('nightmare', 1),\n",
              " ('telling', 1),\n",
              " ('slammed', 1),\n",
              " ('furiously', 1),\n",
              " ('knife', 1),\n",
              " ('ridiculous', 1),\n",
              " ('possibly', 1),\n",
              " ('gotten', 1),\n",
              " ('finger', 1),\n",
              " ('glaring', 1),\n",
              " ('suspiciously', 1),\n",
              " ('listened', 1),\n",
              " ('piece', 1),\n",
              " ('paper', 1),\n",
              " ('stunned', 1),\n",
              " ('white', 1),\n",
              " ('person', 1),\n",
              " ('mouse', 1),\n",
              " ('holes', 1),\n",
              " ('fired', 1),\n",
              " ('lonely', 1),\n",
              " ('woken', 1),\n",
              " ('thoughtfully', 1),\n",
              " ('kill', 1),\n",
              " ('total', 1),\n",
              " ('furious', 1),\n",
              " ('security', 1),\n",
              " ('fast', 1),\n",
              " ('asleep', 1),\n",
              " ('resting', 1),\n",
              " ('joking', 1),\n",
              " ('heading', 1),\n",
              " ('freedom', 1),\n",
              " ('sentence', 1),\n",
              " ('strode', 1),\n",
              " ('banging', 1),\n",
              " ('prefect', 1),\n",
              " ('crackling', 1),\n",
              " ('carrying', 1),\n",
              " ('fine', 1),\n",
              " ('worry', 1),\n",
              " ('feels', 1),\n",
              " ('normal', 1),\n",
              " ('briefly', 1),\n",
              " ('blast', 1),\n",
              " ('knocked', 1),\n",
              " ('backward', 1),\n",
              " ('wrenched', 1),\n",
              " ('dozen', 1),\n",
              " ('allow', 1),\n",
              " ('cornered', 1),\n",
              " ('brothers', 1),\n",
              " ('frantically', 1),\n",
              " ('resolutely', 1),\n",
              " ('hello', 1),\n",
              " ('instead', 1),\n",
              " ('far', 1),\n",
              " ('badges', 1),\n",
              " ('minute', 1),\n",
              " ('keeping', 1),\n",
              " ('dare', 1),\n",
              " ('slow', 1),\n",
              " ('gasped', 1),\n",
              " ('muttered', 1),\n",
              " ('sleepily', 1),\n",
              " ('opening', 1),\n",
              " ('fourth', 1),\n",
              " ('bowed', 1),\n",
              " ('parvati', 1),\n",
              " ('action', 1),\n",
              " ('wishing', 1),\n",
              " ('winked', 1),\n",
              " ('o', 1),\n",
              " ('fool', 1),\n",
              " ('cho', 1),\n",
              " ('lights', 1),\n",
              " ('irritated', 1),\n",
              " ('dragons', 1),\n",
              " ('bye', 1),\n",
              " ('straight', 1),\n",
              " ('tortured', 1),\n",
              " ('size', 1),\n",
              " ('unpleasant', 1),\n",
              " ('month', 1),\n",
              " ('sticking', 1),\n",
              " ('charm', 1),\n",
              " ('quick', 1),\n",
              " ('bewildered', 1),\n",
              " ('earsplitting', 1),\n",
              " ('lupin', 1),\n",
              " ('calm', 1),\n",
              " ('kitchen', 1),\n",
              " ('seat', 1),\n",
              " ('obviously', 1),\n",
              " ('walking', 1),\n",
              " ('clattering', 1),\n",
              " ('chain', 1),\n",
              " ('deep', 1),\n",
              " ('orders', 1),\n",
              " ('foul', 1),\n",
              " ('hopefully', 1),\n",
              " ('parents', 1),\n",
              " ('sighed', 1),\n",
              " ('wouldn', 1),\n",
              " ('occasionally', 1),\n",
              " ('useful', 1),\n",
              " ('stuffed', 1),\n",
              " ('cage', 1),\n",
              " ('dragging', 1),\n",
              " ('trunk', 1),\n",
              " ('howling', 1),\n",
              " ('rage', 1),\n",
              " ('bothering', 1),\n",
              " ('close', 1),\n",
              " ('bound', 1),\n",
              " ('events', 1),\n",
              " ('graveyard', 1),\n",
              " ('er', 1),\n",
              " ('glumly', 1),\n",
              " ('positively', 1),\n",
              " ('alarmed', 1),\n",
              " ('cabin', 1),\n",
              " ('chairs', 1),\n",
              " ('sense', 1),\n",
              " ('stuff', 1),\n",
              " ('working', 1),\n",
              " ('carefully', 1),\n",
              " ('owlery', 1),\n",
              " ('headless', 1),\n",
              " ('nick', 1),\n",
              " ('drifting', 1),\n",
              " ('coolly', 1),\n",
              " ('hour', 1),\n",
              " ('lousy', 1),\n",
              " ('disheveled', 1),\n",
              " ('realize', 1),\n",
              " ('happen', 1),\n",
              " ('fair', 1),\n",
              " ('giggling', 1),\n",
              " ('madly', 1),\n",
              " ('fashioned', 1),\n",
              " ('senses', 1),\n",
              " ('early', 1),\n",
              " ('ravenclaws', 1),\n",
              " ('west', 1),\n",
              " ('finally', 1),\n",
              " ('creaking', 1),\n",
              " ('pale', 1),\n",
              " ('tracks', 1),\n",
              " ('elf', 1),\n",
              " ('hats', 1),\n",
              " ('defensively', 1),\n",
              " ('clicked', 1),\n",
              " ('tongue', 1),\n",
              " ('grown', 1),\n",
              " ('crouch', 1),\n",
              " ('prevent', 1),\n",
              " ('panic', 1),\n",
              " ('strange', 1),\n",
              " ('instrument', 1),\n",
              " ('shout', 1),\n",
              " ('reappeared', 1),\n",
              " ('news', 1),\n",
              " ('doesn', 1),\n",
              " ('blood', 1),\n",
              " ('coughing', 1),\n",
              " ('armchair', 1),\n",
              " ('thank', 1),\n",
              " ('minerva', 1),\n",
              " ('blue', 1),\n",
              " ('quivered', 1),\n",
              " ('marched', 1),\n",
              " ('beard', 1),\n",
              " ('colors', 1),\n",
              " ('jerk', 1),\n",
              " ('wide', 1),\n",
              " ('giving', 1),\n",
              " ('fake', 1),\n",
              " ('eyeing', 1),\n",
              " ('apprehensively', 1),\n",
              " ('destroyed', 1),\n",
              " ('family', 1),\n",
              " ('knows', 1),\n",
              " ('destroy', 1),\n",
              " ('before:', 1),\n",
              " ('issuing', 1),\n",
              " ('bored', 1),\n",
              " ('disappeared', 1),\n",
              " ('antidotes', 1),\n",
              " ('anti', 1),\n",
              " ('unless', 1),\n",
              " ('qualified', 1),\n",
              " ('witch', 1),\n",
              " ('young', 1),\n",
              " ('performing', 1),\n",
              " ('spot', 1),\n",
              " ('crystal', 1),\n",
              " ('bubbles', 1),\n",
              " ('ceiling', 1),\n",
              " ('vicious', 1),\n",
              " ('pace', 1),\n",
              " ('beds', 1),\n",
              " ('brain', 1),\n",
              " ('questions', 1),\n",
              " ('dreadful', 1),\n",
              " ('ideas', 1),\n",
              " ('snake', 1),\n",
              " ('animagus', 1),\n",
              " ('leaning', 1),\n",
              " ('watching', 1),\n",
              " ('squinting', 1),\n",
              " ('outline', 1),\n",
              " ('occurred', 1),\n",
              " ('probably', 1),\n",
              " ('case', 1),\n",
              " ('attacked', 1),\n",
              " ('dad', 1),\n",
              " ('visited', 1),\n",
              " ('comfort', 1),\n",
              " ('pile', 1),\n",
              " ('rat', 1),\n",
              " ('dropping', 1),\n",
              " ('passing', 1),\n",
              " ('says', 1),\n",
              " ('raising', 1),\n",
              " ('eyebrows', 1),\n",
              " ('interesting', 1),\n",
              " ('roared', 1),\n",
              " ('nosed', 1),\n",
              " ('ministry', 1),\n",
              " ('creatures', 1),\n",
              " ('examining', 1),\n",
              " ('unicorns', 1),\n",
              " ('thoroughly', 1),\n",
              " ('tempered', 1),\n",
              " ('runes', 1),\n",
              " ('exams', 1),\n",
              " ('celebration', 1),\n",
              " ('approaching', 1),\n",
              " ('occasional', 1),\n",
              " ('grunt', 1),\n",
              " ('sleeping', 1),\n",
              " ('surroundings', 1),\n",
              " ('reflected', 1),\n",
              " ('feelings', 1),\n",
              " ('pain', 1),\n",
              " ('quiet', 1),\n",
              " ('shattered', 1),\n",
              " ('pieces', 1),\n",
              " ('yells', 1),\n",
              " ('anger', 1),\n",
              " ('fright', 1),\n",
              " ('snatching', 1),\n",
              " ('hitting', 1),\n",
              " ('noticing', 1),\n",
              " ('start', 1),\n",
              " ('cutting', 1),\n",
              " ('telephone', 1),\n",
              " ('heart', 1),\n",
              " ('sank', 1),\n",
              " ('afraid', 1),\n",
              " ('hoping', 1),\n",
              " ('speak', 1),\n",
              " ('dreaming', 1),\n",
              " ('cough', 1),\n",
              " ('magic', 1),\n",
              " ('strain', 1),\n",
              " ('caused', 1),\n",
              " ('utterly', 1),\n",
              " ('terrified', 1),\n",
              " ('self', 1),\n",
              " ('bounced', 1),\n",
              " ('shaken', 1),\n",
              " ('remained', 1),\n",
              " ('encounter', 1),\n",
              " ('given', 1),\n",
              " ('private', 1),\n",
              " ('proved', 1),\n",
              " ('impossible', 1),\n",
              " ('remove', 1),\n",
              " ('art', 1),\n",
              " ('happened', 1),\n",
              " ('ago', 1),\n",
              " ('wet', 1),\n",
              " ('state', 1),\n",
              " ('considerable', 1),\n",
              " ('course', 1),\n",
              " ('busy', 1),\n",
              " ('quill', 1),\n",
              " ('catching', 1),\n",
              " ('finishing', 1),\n",
              " ('letter', 1),\n",
              " ('luck', 1),\n",
              " ('maybe', 1),\n",
              " ('scrimgeour', 1),\n",
              " ('success', 1),\n",
              " ('subsided', 1),\n",
              " ('suddenly', 1),\n",
              " ('official', 1),\n",
              " ('muggles', 1),\n",
              " ('meeting', 1),\n",
              " ('enchantment', 1),\n",
              " ('ensure', 1),\n",
              " ('owned', 1),\n",
              " ('pureblood', 1),\n",
              " ('vivid', 1),\n",
              " ('shrieking', 1),\n",
              " ('spitting', 1),\n",
              " ('flashed', 1),\n",
              " ('snapped', 1),\n",
              " ('particularly', 1),\n",
              " ('minuscule', 1),\n",
              " ('muttering', 1),\n",
              " ('promptly', 1),\n",
              " ('scarlet', 1),\n",
              " ('hope', 1),\n",
              " ('goes', 1),\n",
              " ('pair', 1),\n",
              " ('leave', 1),\n",
              " ('proceeded', 1),\n",
              " ('step', 1),\n",
              " ('statue', 1),\n",
              " ('trelawney', 1),\n",
              " ('smelled', 1),\n",
              " ('damp', 1),\n",
              " ('stalked', 1),\n",
              " ('undoubtedly', 1),\n",
              " ('pause', 1),\n",
              " ('worst', 1),\n",
              " ('darkly', 1),\n",
              " ('evening', 1),\n",
              " ('soon', 1),\n",
              " ('sinking', 1),\n",
              " ('mane', 1),\n",
              " ('bushy', 1),\n",
              " ('brown', 1),\n",
              " ('whipping', 1),\n",
              " ('unlocked', 1),\n",
              " ('classroom', 1),\n",
              " ('hi', 1),\n",
              " ('fancy', 1),\n",
              " ('thanks', 1),\n",
              " ('precisely', 1),\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBFQ3HvSsUDj"
      },
      "source": [
        "How about the word 'ghost'?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faM3y0C0sUDk",
        "outputId": "937d3e5f-951a-41a9-a7c8-3649f8945c40"
      },
      "source": [
        "sorted([(item, value) for item, value in contexts[\"ghost\"].items()], key=lambda x: x[1], reverse=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('harry', 52),\n",
              " ('said', 36),\n",
              " ('nick', 22),\n",
              " ('t', 20),\n",
              " ('nearly', 20),\n",
              " ('headless', 18),\n",
              " ('know', 15),\n",
              " ('looked', 12),\n",
              " ('ron', 10),\n",
              " ('saw', 9),\n",
              " ('d', 9),\n",
              " ('ve', 9),\n",
              " ('got', 8),\n",
              " ('gryffindor', 8),\n",
              " ('years', 7),\n",
              " ('bloody', 7),\n",
              " ('baron', 7),\n",
              " ('wand', 7),\n",
              " ('cedric', 7),\n",
              " ('dumbledore', 7),\n",
              " ('just', 6),\n",
              " ('don', 6),\n",
              " ('think', 6),\n",
              " ('told', 6),\n",
              " ('slytherin', 6),\n",
              " ('staring', 6),\n",
              " ('eyes', 6),\n",
              " ('professor', 6),\n",
              " ('passed', 6),\n",
              " ('like', 6),\n",
              " ('hermione', 6),\n",
              " ('gray', 6),\n",
              " ('ruff', 5),\n",
              " ('suddenly', 5),\n",
              " ('table', 5),\n",
              " ('potter', 5),\n",
              " ('giving', 5),\n",
              " ('began', 5),\n",
              " ('tower', 5),\n",
              " ('magic', 5),\n",
              " ('binns', 5),\n",
              " ('light', 5),\n",
              " ('held', 5),\n",
              " ('head', 5),\n",
              " ('little', 5),\n",
              " ('solid', 5),\n",
              " ('really', 4),\n",
              " ('hufflepuff', 4),\n",
              " ('hat', 4),\n",
              " ('opposite', 4),\n",
              " ('seen', 4),\n",
              " ('arm', 4),\n",
              " ('need', 4),\n",
              " ('sir', 4),\n",
              " ('gaunt', 4),\n",
              " ('face', 4),\n",
              " ('silver', 4),\n",
              " ('corridor', 4),\n",
              " ('covered', 4),\n",
              " ('wide', 4),\n",
              " ('oh', 4),\n",
              " ('asked', 4),\n",
              " ('away', 4),\n",
              " ('myrtle', 4),\n",
              " ('hair', 4),\n",
              " ('thing', 4),\n",
              " ('hogwarts', 4),\n",
              " ('white', 4),\n",
              " ('stood', 4),\n",
              " ('voldemort', 4),\n",
              " ('end', 4),\n",
              " ('neville', 4),\n",
              " ('come', 4),\n",
              " ('haven', 3),\n",
              " ('peeves', 3),\n",
              " ('say', 3),\n",
              " ('doing', 3),\n",
              " ('wearing', 3),\n",
              " ('fat', 3),\n",
              " ('sat', 3),\n",
              " ('horrible', 3),\n",
              " ('plate', 3),\n",
              " ('good', 3),\n",
              " ('course', 3),\n",
              " ('seamus', 3),\n",
              " ('sitting', 3),\n",
              " ('history', 3),\n",
              " ('taught', 3),\n",
              " ('let', 3),\n",
              " ('direction', 3),\n",
              " ('moaning', 3),\n",
              " ('came', 3),\n",
              " ('help', 3),\n",
              " ('hedwig', 3),\n",
              " ('window', 3),\n",
              " ('man', 3),\n",
              " ('talking', 3),\n",
              " ('wasn', 3),\n",
              " ('surprised', 3),\n",
              " ('drifted', 3),\n",
              " ('mean', 3),\n",
              " ('girl', 3),\n",
              " ('wall', 3),\n",
              " ('floor', 3),\n",
              " ('large', 3),\n",
              " ('room', 3),\n",
              " ('attack', 3),\n",
              " ('crash', 3),\n",
              " ('gryffindors', 3),\n",
              " ('ravenclaw', 3),\n",
              " ('riddle', 3),\n",
              " ('diary', 3),\n",
              " ('diggory', 3),\n",
              " ('looking', 3),\n",
              " ('heard', 3),\n",
              " ('pain', 3),\n",
              " ('body', 3),\n",
              " ('alive', 3),\n",
              " ('gone', 3),\n",
              " ('golden', 3),\n",
              " ('emerged', 3),\n",
              " ('wizard', 3),\n",
              " ('malfoy', 3),\n",
              " ('sure', 3),\n",
              " ('given', 2),\n",
              " ('chances', 2),\n",
              " ('deserves', 2),\n",
              " ('gives', 2),\n",
              " ('bad', 2),\n",
              " ('noticed', 2),\n",
              " ('answered', 2),\n",
              " ('students', 2),\n",
              " ('clapped', 2),\n",
              " ('susan', 2),\n",
              " ('shouted', 2),\n",
              " ('weasley', 2),\n",
              " ('twins', 2),\n",
              " ('earlier', 2),\n",
              " ('patted', 2),\n",
              " ('sudden', 2),\n",
              " ('bit', 2),\n",
              " ('does', 2),\n",
              " ('sadly', 2),\n",
              " ('cut', 2),\n",
              " ('steak', 2),\n",
              " ('eaten', 2),\n",
              " ('myself', 2),\n",
              " ('brothers', 2),\n",
              " ('haired', 2),\n",
              " ('finnigan', 2),\n",
              " ('blank', 2),\n",
              " ('used', 2),\n",
              " ('boring', 2),\n",
              " ('class', 2),\n",
              " ('old', 2),\n",
              " ('m', 2),\n",
              " ('tall', 2),\n",
              " ('gliding', 2),\n",
              " ('started', 2),\n",
              " ('free', 2),\n",
              " ('soft', 2),\n",
              " ('ahead', 2),\n",
              " ('wings', 2),\n",
              " ('moment', 2),\n",
              " ('soared', 2),\n",
              " ('impatiently', 2),\n",
              " ('warning', 2),\n",
              " ('somebody', 2),\n",
              " ('forehead', 2),\n",
              " ('watched', 2),\n",
              " ('crouched', 2),\n",
              " ('walked', 2),\n",
              " ('er', 2),\n",
              " ('horses', 2),\n",
              " ('pack', 2),\n",
              " ('bearded', 2),\n",
              " ('position', 2),\n",
              " ('blowing', 2),\n",
              " ('horn', 2),\n",
              " ('leapt', 2),\n",
              " ('lifted', 2),\n",
              " ('high', 2),\n",
              " ('laughed', 2),\n",
              " ('subject', 2),\n",
              " ('teacher', 2),\n",
              " ('happened', 2),\n",
              " ('way', 2),\n",
              " ('filled', 2),\n",
              " ('safe', 2),\n",
              " ('door', 2),\n",
              " ('flew', 2),\n",
              " ('people', 2),\n",
              " ('dead', 2),\n",
              " ('seats', 2),\n",
              " ('counting', 2),\n",
              " ('school', 2),\n",
              " ('ago', 2),\n",
              " ('memory', 2),\n",
              " ('quietly', 2),\n",
              " ('rolling', 2),\n",
              " ('sleeves', 2),\n",
              " ('turn', 2),\n",
              " ('black', 2),\n",
              " ('quickly', 2),\n",
              " ('seeing', 2),\n",
              " ('things', 2),\n",
              " ('wands', 2),\n",
              " ('shadow', 2),\n",
              " ('skull', 2),\n",
              " ('mr', 2),\n",
              " ('vanished', 2),\n",
              " ('rest', 2),\n",
              " ('particularly', 2),\n",
              " ('chance', 2),\n",
              " ('opinion', 2),\n",
              " ('foot', 2),\n",
              " ('running', 2),\n",
              " ('clutching', 2),\n",
              " ('fear', 2),\n",
              " ('going', 2),\n",
              " ('water', 2),\n",
              " ('shock', 2),\n",
              " ('ripped', 2),\n",
              " ('spirit', 2),\n",
              " ('hand', 2),\n",
              " ('tip', 2),\n",
              " ('tightly', 2),\n",
              " ('thread', 2),\n",
              " ('remained', 2),\n",
              " ('itself', 2),\n",
              " ('correct', 2),\n",
              " ('spoke', 2),\n",
              " ('shaking', 2),\n",
              " ('past', 2),\n",
              " ('feels', 2),\n",
              " ('voice', 2),\n",
              " ('hey', 2),\n",
              " ('cloak', 2),\n",
              " ('floating', 2),\n",
              " ('board', 2),\n",
              " ('long', 2),\n",
              " ('tried', 2),\n",
              " ('tell', 2),\n",
              " ('difference', 2),\n",
              " ('lying', 2),\n",
              " ('tomb', 2),\n",
              " ('dunno', 2),\n",
              " ('wouldn', 2),\n",
              " ('felt', 2),\n",
              " ('length', 2),\n",
              " ('times', 2),\n",
              " ('spoken', 2),\n",
              " ('lady', 2),\n",
              " ('nodded', 2),\n",
              " ('did', 2),\n",
              " ('speak', 2),\n",
              " ('fred', 2),\n",
              " ('new', 1),\n",
              " ('right', 1),\n",
              " ('cheered', 1),\n",
              " ('hannah', 1),\n",
              " ('went', 1),\n",
              " ('sit', 1),\n",
              " ('waving', 1),\n",
              " ('merrily', 1),\n",
              " ('bones', 1),\n",
              " ('yelled', 1),\n",
              " ('feeling', 1),\n",
              " ('plunged', 1),\n",
              " ('eat', 1),\n",
              " ('delicious', 1),\n",
              " ('look', 1),\n",
              " ('watching', 1),\n",
              " ('miss', 1),\n",
              " ('introduced', 1),\n",
              " ('service', 1),\n",
              " ('prefer', 1),\n",
              " ('stiffly', 1),\n",
              " ('interrupted', 1),\n",
              " ('winning', 1),\n",
              " ('slytherins', 1),\n",
              " ('cup', 1),\n",
              " ('row', 1),\n",
              " ('robes', 1),\n",
              " ('stained', 1),\n",
              " ('blood', 1),\n",
              " ('strange', 1),\n",
              " ('plants', 1),\n",
              " ('easily', 1),\n",
              " ('fallen', 1),\n",
              " ('asleep', 1),\n",
              " ('staffroom', 1),\n",
              " ('morning', 1),\n",
              " ('teach', 1),\n",
              " ('leaving', 1),\n",
              " ('freezing', 1),\n",
              " ('forget', 1),\n",
              " ('hissed', 1),\n",
              " ('witch', 1),\n",
              " ('stirring', 1),\n",
              " ('cauldrons', 1),\n",
              " ('wonderful', 1),\n",
              " ('week', 1),\n",
              " ('exam', 1),\n",
              " ('results', 1),\n",
              " ('quills', 1),\n",
              " ('roll', 1),\n",
              " ('parchment', 1),\n",
              " ('couldn', 1),\n",
              " ('whispered', 1),\n",
              " ('listened', 1),\n",
              " ('rustling', 1),\n",
              " ('clinking', 1),\n",
              " ('coming', 1),\n",
              " ('sounds', 1),\n",
              " ('moving', 1),\n",
              " ('reached', 1),\n",
              " ('later', 1),\n",
              " ('alongside', 1),\n",
              " ('story', 1),\n",
              " ('happening', 1),\n",
              " ('dobby', 1),\n",
              " ('deserted', 1),\n",
              " ('muttering', 1),\n",
              " ('breath', 1),\n",
              " ('gloomy', 1),\n",
              " ('ragged', 1),\n",
              " ('chains', 1),\n",
              " ('cheerful', 1),\n",
              " ('knight', 1),\n",
              " ('sticking', 1),\n",
              " ('ghosts', 1),\n",
              " ('died', 1),\n",
              " ('october', 1),\n",
              " ('amazed', 1),\n",
              " ('approached', 1),\n",
              " ('low', 1),\n",
              " ('mouth', 1),\n",
              " ('taste', 1),\n",
              " ('walk', 1),\n",
              " ('expect', 1),\n",
              " ('stronger', 1),\n",
              " ('flavor', 1),\n",
              " ('didn', 1),\n",
              " ('mind', 1),\n",
              " ('hello', 1),\n",
              " ('squat', 1),\n",
              " ('glided', 1),\n",
              " ('half', 1),\n",
              " ('hidden', 1),\n",
              " ('bitterly', 1),\n",
              " ('dungeon', 1),\n",
              " ('burst', 1),\n",
              " ('dozen', 1),\n",
              " ('wildly', 1),\n",
              " ('clap', 1),\n",
              " ('middle', 1),\n",
              " ('dance', 1),\n",
              " ('halted', 1),\n",
              " ('plunging', 1),\n",
              " ('air', 1),\n",
              " ('crowd', 1),\n",
              " ('strode', 1),\n",
              " ('schedule', 1),\n",
              " ('exciting', 1),\n",
              " ('classes', 1),\n",
              " ('entering', 1),\n",
              " ('blackboard', 1),\n",
              " ('ancient', 1),\n",
              " ('shriveled', 1),\n",
              " ('lungs', 1),\n",
              " ('stop', 1),\n",
              " ('screamed', 1),\n",
              " ('mortal', 1),\n",
              " ('run', 1),\n",
              " ('lives', 1),\n",
              " ('real', 1),\n",
              " ('panic', 1),\n",
              " ('curiously', 1),\n",
              " ('fate', 1),\n",
              " ('worry', 1),\n",
              " ('possibly', 1),\n",
              " ('terrible', 1),\n",
              " ('power', 1),\n",
              " ('harm', 1),\n",
              " ('book', 1),\n",
              " ('awkwardly', 1),\n",
              " ('portrait', 1),\n",
              " ('hole', 1),\n",
              " ('immediately', 1),\n",
              " ('friend', 1),\n",
              " ('lee', 1),\n",
              " ('jordan', 1),\n",
              " ('fingers', 1),\n",
              " ('stand', 1),\n",
              " ('lockhart', 1),\n",
              " ('getting', 1),\n",
              " ('feet', 1),\n",
              " ('ways', 1),\n",
              " ('aside', 1),\n",
              " ('pipe', 1),\n",
              " ('miles', 1),\n",
              " ('weird', 1),\n",
              " ('misty', 1),\n",
              " ('shining', 1),\n",
              " ('day', 1),\n",
              " ('older', 1),\n",
              " ('sixteen', 1),\n",
              " ('uncertainly', 1),\n",
              " ('fifty', 1),\n",
              " ('popped', 1),\n",
              " ('walls', 1),\n",
              " ('tables', 1),\n",
              " ('great', 1),\n",
              " ('success', 1),\n",
              " ('pleasant', 1),\n",
              " ('evening', 1),\n",
              " ('mood', 1),\n",
              " ('lupin', 1),\n",
              " ('forgive', 1),\n",
              " ('believing', 1),\n",
              " ('spy', 1),\n",
              " ('grin', 1),\n",
              " ('flitted', 1),\n",
              " ('shall', 1),\n",
              " ('kill', 1),\n",
              " ('dad', 1),\n",
              " ('maybe', 1),\n",
              " ('met', 1),\n",
              " ('mere', 1),\n",
              " ('green', 1),\n",
              " ('spell', 1),\n",
              " ('smoke', 1),\n",
              " ('hufflepuffs', 1),\n",
              " ('far', 1),\n",
              " ('hall', 1),\n",
              " ('pearly', 1),\n",
              " ('dressed', 1),\n",
              " ('tonight', 1),\n",
              " ('usual', 1),\n",
              " ('question', 1),\n",
              " ('utterly', 1),\n",
              " ('food', 1),\n",
              " ('throwing', 1),\n",
              " ('silent', 1),\n",
              " ('person', 1),\n",
              " ('control', 1),\n",
              " ('store', 1),\n",
              " ('amused', 1),\n",
              " ('month', 1),\n",
              " ('ideas', 1),\n",
              " ('writing', 1),\n",
              " ('weekly', 1),\n",
              " ('goblin', 1),\n",
              " ('century', 1),\n",
              " ('amazing', 1),\n",
              " ('seriously', 1),\n",
              " ('goblet', 1),\n",
              " ('reckon', 1),\n",
              " ('trying', 1),\n",
              " ('weeks', 1),\n",
              " ('meeting', 1),\n",
              " ('noise', 1),\n",
              " ('loud', 1),\n",
              " ('wailing', 1),\n",
              " ('nearest', 1),\n",
              " ('party', 1),\n",
              " ('playing', 1),\n",
              " ('musical', 1),\n",
              " ('shut', 1),\n",
              " ('miracle', 1),\n",
              " ('sort', 1),\n",
              " ('extra', 1),\n",
              " ('concentrated', 1),\n",
              " ('supposed', 1),\n",
              " ('starting', 1),\n",
              " ('dancing', 1),\n",
              " ('champions', 1),\n",
              " ('suppose', 1),\n",
              " ('gloomily', 1),\n",
              " ('girls', 1),\n",
              " ('second', 1),\n",
              " ('putting', 1),\n",
              " ('swallowed', 1),\n",
              " ('considerable', 1),\n",
              " ('bubbles', 1),\n",
              " ('cross', 1),\n",
              " ('legged', 1),\n",
              " ('taps', 1),\n",
              " ('usually', 1),\n",
              " ('friends', 1),\n",
              " ('prepared', 1),\n",
              " ('anybody', 1),\n",
              " ('path', 1),\n",
              " ('leads', 1),\n",
              " ('goal', 1),\n",
              " ('red', 1),\n",
              " ('widened', 1),\n",
              " ('dense', 1),\n",
              " ('wormtail', 1),\n",
              " ('shouts', 1),\n",
              " ('larger', 1),\n",
              " ('kept', 1),\n",
              " ('squeezing', 1),\n",
              " ('narrow', 1),\n",
              " ('dream', 1),\n",
              " ('pushing', 1),\n",
              " ('himself', 1),\n",
              " ('fell', 1),\n",
              " ('surveyed', 1),\n",
              " ('web', 1),\n",
              " ('connected', 1),\n",
              " ('appearance', 1),\n",
              " ('guessing', 1),\n",
              " ('limped', 1),\n",
              " ('rustle', 1),\n",
              " ('small', 1),\n",
              " ('time', 1),\n",
              " ('snarled', 1),\n",
              " ('landed', 1),\n",
              " ('lightly', 1),\n",
              " ('cage', 1),\n",
              " ('work', 1),\n",
              " ('halfway', 1),\n",
              " ('house', 1),\n",
              " ('parvati', 1),\n",
              " ('patil', 1),\n",
              " ('lavender', 1),\n",
              " ('brown', 1),\n",
              " ('gave', 1),\n",
              " ('friendly', 1),\n",
              " ('leaning', 1),\n",
              " ('winced', 1),\n",
              " ('uncomfortable', 1),\n",
              " ('lean', 1),\n",
              " ('honor', 1),\n",
              " ('bound', 1),\n",
              " ('saying', 1),\n",
              " ('sorting', 1),\n",
              " ('glad', 1),\n",
              " ('reason', 1),\n",
              " ('common', 1),\n",
              " ('kind', 1),\n",
              " ('wheezy', 1),\n",
              " ('cause', 1),\n",
              " ('severe', 1),\n",
              " ('minutes', 1),\n",
              " ('warm', 1),\n",
              " ('drifting', 1),\n",
              " ('stuck', 1),\n",
              " ('revealing', 1),\n",
              " ('dangerously', 1),\n",
              " ('continued', 1),\n",
              " ('hesitated', 1),\n",
              " ('wizards', 1),\n",
              " ('year', 1),\n",
              " ('pansy', 1),\n",
              " ('indignantly', 1),\n",
              " ('smirk', 1),\n",
              " ('moved', 1),\n",
              " ('bigger', 1),\n",
              " ('better', 1),\n",
              " ('luggage', 1),\n",
              " ('rack', 1),\n",
              " ('conscious', 1),\n",
              " ('ginny', 1),\n",
              " ('dean', 1),\n",
              " ('listening', 1),\n",
              " ('bench', 1),\n",
              " ('darkly', 1),\n",
              " ('silvery', 1),\n",
              " ('mass', 1),\n",
              " ('rose', 1),\n",
              " ('revolving', 1),\n",
              " ('slowly', 1),\n",
              " ('pensieve', 1),\n",
              " ('completely', 1),\n",
              " ('curious', 1),\n",
              " ('circumstances', 1),\n",
              " ('brought', 1),\n",
              " ('yeh', 1),\n",
              " ('o', 1),\n",
              " ('governors', 1),\n",
              " ('hagrid', 1),\n",
              " ('stopped', 1),\n",
              " ('woman', 1),\n",
              " ('serenely', 1),\n",
              " ('resumed', 1),\n",
              " ('hoarse', 1),\n",
              " ('whisper', 1),\n",
              " ('corner', 1),\n",
              " ('apparently', 1),\n",
              " ('impression', 1),\n",
              " ('draco', 1),\n",
              " ('inside', 1),\n",
              " ('hour', 1),\n",
              " ('wondering', 1),\n",
              " ('paper', 1),\n",
              " ('snape', 1),\n",
              " ('bored', 1),\n",
              " ('fixed', 1),\n",
              " ('ask', 1),\n",
              " ('hastily', 1),\n",
              " ('recall', 1),\n",
              " ('night', 1),\n",
              " ('dark', 1),\n",
              " ('spells', 1),\n",
              " ('merely', 1),\n",
              " ('bidding', 1),\n",
              " ('trust', 1),\n",
              " ('aware', 1),\n",
              " ('departed', 1),\n",
              " ('soul', 1),\n",
              " ('left', 1),\n",
              " ('earth', 1),\n",
              " ('test', 1),\n",
              " ('boys', 1),\n",
              " ('bathroom', 1),\n",
              " ('risen', 1),\n",
              " ('toilet', 1),\n",
              " ('midair', 1),\n",
              " ('round', 1),\n",
              " ('glasses', 1),\n",
              " ('remembering', 1),\n",
              " ('words', 1),\n",
              " ('before:', 1),\n",
              " ('want', 1),\n",
              " ('tom', 1),\n",
              " ('death', 1),\n",
              " ('apparent', 1),\n",
              " ('expression', 1),\n",
              " ('explain', 1),\n",
              " ('sent', 1),\n",
              " ('knew', 1),\n",
              " ('precious', 1),\n",
              " ('attacked', 1),\n",
              " ('true', 1),\n",
              " ('destroyed', 1),\n",
              " ('thought', 1),\n",
              " ('feel', 1),\n",
              " ('surely', 1),\n",
              " ('horcruxes', 1),\n",
              " ('paced', 1),\n",
              " ('luna', 1),\n",
              " ('checking', 1),\n",
              " ('marauder', 1),\n",
              " ('map', 1),\n",
              " ('permitted', 1),\n",
              " ('twice', 1),\n",
              " ('pausing', 1),\n",
              " ('allow', 1),\n",
              " ('pass', 1),\n",
              " ('drawing', 1),\n",
              " ('attention', 1),\n",
              " ('expected', 1),\n",
              " ('encounter', 1),\n",
              " ('worst', 1),\n",
              " ('forced', 1),\n",
              " ('finally', 1),\n",
              " ('reaching', 1),\n",
              " ('stairs', 1),\n",
              " ('waiting', 1),\n",
              " ('dear', 1),\n",
              " ('boy', 1),\n",
              " ('grasp', 1),\n",
              " ('thrust', 1),\n",
              " ('icy', 1),\n",
              " ('offended', 1),\n",
              " ('transparent', 1),\n",
              " ('pointing', 1),\n",
              " ('finger', 1),\n",
              " ('caught', 1),\n",
              " ('sight', 1),\n",
              " ('raised', 1),\n",
              " ('eyebrows', 1),\n",
              " ('proud', 1),\n",
              " ('close', 1),\n",
              " ('recognized', 1),\n",
              " ('tone', 1),\n",
              " ('percy', 1),\n",
              " ('brother', 1),\n",
              " ('kneeling', 1),\n",
              " ('stared', 1),\n",
              " ('laugh', 1),\n",
              " ('etched', 1),\n",
              " ('chapter', 1),\n",
              " ('thirty', 1),\n",
              " ('elder', 1),\n",
              " ('world', 1),\n",
              " ('ended', 1),\n",
              " ('battle', 1),\n",
              " ('twig', 1),\n",
              " ('strewn', 1),\n",
              " ('ground', 1),\n",
              " ('marked', 1),\n",
              " ('outer', 1),\n",
              " ('edge', 1),\n",
              " ('forest', 1),\n",
              " ('opened', 1),\n",
              " ('truly', 1),\n",
              " ('flesh', 1),\n",
              " ('resembled', 1),\n",
              " ('closely', 1),\n",
              " ('escaped', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3_IGbPisUDk"
      },
      "source": [
        "The co-occurrence matrix of a very large corpus should give a meaningful summary of how a word is used in general. A single row of that matrix is already a __word vector__ of size $N$. However such vectors are extremely sparse, and for large corpora the size of $N$ will become unwieldy. We will follow along the paper in designing an algorithm that can compress the word vectors while retaining most of their informational content. \n",
        "\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "<b>Note:</b>\n",
        "For the resulting vectors to actually be informative, the source corpus should have a size of at least a few billion words; on the contrary, our corpus enumerates merely a million words, so we can't expect our results to be as great.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDeZDOhwsUDk"
      },
      "source": [
        "### Sparsity and Stability\n",
        "\n",
        "Our matrix $X$ is very sparse; most of its elements are zero.\n",
        "\n",
        "**Coding 1.1**: Find what the ratio of non-zero elements is.\n",
        "\n",
        "_Hint_: The function `non_zero_ratio` should return a `float` rather than a `FloatTensor`. Remember `.item()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuyvalzWsUDl",
        "outputId": "c08b46b1-028d-4e56-d112-ef4acd12b3f8"
      },
      "source": [
        "def non_zero_ratio(sparse_matrix: LongTensor) -> float:\n",
        "    non_zero = torch.count_nonzero(sparse_matrix)\n",
        "    total = torch.numel(sparse_matrix)\n",
        "    return  non_zero.item() / total\n",
        "\n",
        "non_zero_ratio(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk8M2IAwsUDl"
      },
      "source": [
        "We will soon need to perform division and find the logarithm of ${X}$. Neither of the two operations are well-defined for $0$. That's why for further processing we want to have a matrix without any zero elements. \n",
        "\n",
        "**Coding 1.2**: Change the matrix's datatype to a `torch.float` and add a small constant to it (e.g. $0.1$) to ensure numerical stability while maintaining sparsity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSxbSzPP2ZaN"
      },
      "source": [
        "X = X + 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQ-8P48fsUDl",
        "outputId": "218d8b8e-57e7-4d27-ff03-f78e62ef5161"
      },
      "source": [
        "print(X.dtype)\n",
        "non_zero_ratio(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float32\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2KuQeuBsUDl"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>Show the completed code to your teacher before proceeding</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtZuVAKNsUDm"
      },
      "source": [
        "### From co-occurrence counts to probabilities\n",
        "From the paper: \n",
        "> Let the matrix of word-word co-occurrence counts be denoted by $X$, whose entries $X_{ij}$ tabulate the number of times word $j$ occurs in the context of word $i$.  Let $X_i$= $\\sum_{k} X_{ik}$ be the number of times any word appears in the context of word $i$. Finally, let $P_{ij} = P(j  | i) =  X_{ij}/X_i$ be the probability that word $j$ appear in the context of word $i$. \n",
        "\n",
        "**Coding 2**: Complete the function `to_probabilities` that accepts a co-occurrence matrix and returns the probability matrix $P$. \n",
        "\n",
        "_Hint_: Remember broadcasting and `torch.sum()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3wj3iCW8oSy",
        "outputId": "521fa98b-a199-496a-e369-a8bd7e75e69e"
      },
      "source": [
        "def to_probabilities(count_matrix: FloatTensor) -> FloatTensor:\n",
        "      sum = torch.sum(count_matrix, dim=1)\n",
        "      return count_matrix / sum[:, None]\n",
        "P = to_probabilities(X)\n",
        "print(P)\n",
        "print(P.shape)\n",
        "\n",
        "A = torch.tensor([[1., 2., 3.], [1., -3., 4]])\n",
        "print(to_probabilities(A))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.2439e-04, 1.2439e-04, 1.2439e-04,  ..., 1.2439e-04, 1.2439e-04,\n",
            "         1.2439e-04],\n",
            "        [4.5067e-05, 4.5067e-05, 4.5067e-05,  ..., 4.5067e-05, 4.5067e-05,\n",
            "         4.5067e-05],\n",
            "        [9.7191e-05, 9.7191e-05, 9.7191e-05,  ..., 9.7191e-05, 9.7191e-05,\n",
            "         9.7191e-05],\n",
            "        ...,\n",
            "        [1.3177e-04, 1.3177e-04, 1.3177e-04,  ..., 1.3177e-04, 1.3177e-04,\n",
            "         1.3177e-04],\n",
            "        [1.1602e-04, 1.1602e-04, 1.1602e-04,  ..., 1.1602e-04, 1.1602e-04,\n",
            "         1.1602e-04],\n",
            "        [7.0328e-05, 7.0328e-05, 7.0328e-05,  ..., 7.0328e-05, 7.0328e-05,\n",
            "         7.0328e-05]])\n",
            "torch.Size([5359, 5359])\n",
            "tensor([[ 0.1667,  0.3333,  0.5000],\n",
            "        [ 0.5000, -1.5000,  2.0000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCBo8RLmsUDm"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>Show the completed code to your teacher before proceeding</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Og1ZRWYSsUDm"
      },
      "source": [
        "### Probing words\n",
        "\n",
        "From the paper:\n",
        "> Consider two words $i$ and $j$ that exhibit a particular aspect of interest. The relationship of these words can be examined by studying the ratio of their co-occurrence probabilities with various probe words, $k$.  For words $k$ related to $i$ but not $j$, we expect the ratio $P_{ik}/P_{jk}$ will be large.  Similarly, for words $k$ related to $j$ but not $i$, the ratio should be small. For words $k$ that are either related to both $i$ and $j$, or to neither, the ratio should be close to one.\n",
        "\n",
        "**Coding 3.1**: Complete the function `query` that accepts two words $w_i$ and $w_j$, a vocab $V$ and a probability matrix ${P}$, maps each word to its corresponding index and returns the probability $P(j  |  i)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dC4j4s3-sUDm"
      },
      "source": [
        "def query(word_i: str, word_j: str, vocab: Dict[str, int], probability_matrix: FloatTensor) -> float:  \n",
        "    i = vocab[word_i]\n",
        "    j = vocab[word_j]\n",
        "    return probability_matrix[i,j].item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUOgM4DbsUDn"
      },
      "source": [
        "**Coding 3.2**: Then, complete the function `probe` that accepts three words $w_i$, $w_j$ and $w_k$, a vocab $V$ and a probability matrix ${P}$, calls `query` and returns the ratio $P(k |  i) / P(k  |  j)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0E7aPwuksUDn"
      },
      "source": [
        "def probe(word_i: str, word_j: str, word_k: str, vocab: Dict[str, int], probability_matrix: FloatTensor) -> float:\n",
        "    Pik = query(word_i, word_k, vocab, probability_matrix)\n",
        "    Pjk = query(word_j, word_k, vocab, probability_matrix)\n",
        "    return Pik / Pjk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJeMz8ijsUDn"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>Show the completed code to your teacher before proceeding</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsickgewsUDn"
      },
      "source": [
        "Let's probe a few words and examine whether the authors' claim holds even for our (tiny) corpus. Feel free to add your own word triplets and experiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "SxHRCXglsUDn",
        "outputId": "70da528f-2ba7-4250-e6ad-d3ae972ac01f"
      },
      "source": [
        "print(\"tea\", \"wand\", \"spell\", probe(\"tea\", \"wand\", \"spell\", vocab, P))\n",
        "print(\"tea\", \"wand\", \"cup\", probe(\"tea\", \"wand\", \"cup\", vocab, P))\n",
        "print()\n",
        "\n",
        "print(\"voldemort\", \"hagrid\", \"curse\", probe(\"voldemort\", \"hagrid\", \"curse\", vocab, P))\n",
        "print(\"voldemort\", \"hagrid\", \"beast\", probe(\"voldemort\", \"hagrid\", \"beast\", vocab, P))\n",
        "print()\n",
        "\n",
        "print(\"mcgonagall\", \"snape\", \"potions\", probe(\"mcgonagall\", \"snape\", \"potions\", vocab, P))\n",
        "print(\"mcgonagall\", \"snape\", \"transfiguration\", probe(\"mcgonagall\", \"snape\", \"transfiguration\", vocab, P))\n",
        "print()\n",
        "\n",
        "print(\"hedwig\", \"scabbers\", \"owl\", probe(\"hedwig\", \"scabbers\", \"owl\", vocab, P))\n",
        "print(\"hedwig\", \"scabbers\", \"rat\", probe(\"hedwig\", \"scabbers\", \"rat\", vocab, P))\n",
        "print()\n",
        "\n",
        "print(\"ron\", \"hermione\", \"book\", probe(\"ron\", \"hermione\", \"book\", vocab, P))\n",
        "print(\"ron\", \"hermione\", \"red\", probe(\"ron\", \"hermione\", \"red\", vocab, P))\n",
        "print()\n",
        "\n",
        "'''\n",
        "print(..., ..., ..., probe(..., ..., ..., vocab, P))\n",
        "print(..., ..., ..., probe(..., ..., ..., vocab, P))\n",
        "print()\n",
        "\n",
        "print(..., ..., ..., probe(..., ..., ..., vocab, P))\n",
        "print(..., ..., ..., probe(..., ..., ..., vocab, P))\n",
        "print()\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tea wand spell 0.4226341599930948\n",
            "tea wand cup 24.613074374771603\n",
            "\n",
            "voldemort hagrid curse 8.71081893708466\n",
            "voldemort hagrid beast 0.5588326759742435\n",
            "\n",
            "mcgonagall snape potions 0.0036196261777071337\n",
            "mcgonagall snape transfiguration 43.71883147330327\n",
            "\n",
            "hedwig scabbers owl 5.62658223137728\n",
            "hedwig scabbers rat 0.01781921270378008\n",
            "\n",
            "ron hermione book 0.6783073675617391\n",
            "ron hermione red 2.038255365612192\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nprint(..., ..., ..., probe(..., ..., ..., vocab, P))\\nprint(..., ..., ..., probe(..., ..., ..., vocab, P))\\nprint()\\n\\nprint(..., ..., ..., probe(..., ..., ..., vocab, P))\\nprint(..., ..., ..., probe(..., ..., ..., vocab, P))\\nprint()\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8iLYyEKsUDo"
      },
      "source": [
        "**Interpretation 1**: Give a brief interpretation of the results you got. Do they correspond to your expectations? Why or why not?\n",
        "\n",
        "*Hint*: When do we expect the ratio value to be high, low or close to 1? Refer to the GloVe paper for guidance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01vHhk9bG6hj"
      },
      "source": [
        "- For example 1, both outcomes are expected. One would expect tea to have a low relation to spell, but the reverse would be true for the relation for spell and wand as spells are casted with a wand. Therefore, the small result is expected as we are dividing a small relation by a big one. The same reasoning holds for the relation of tea and cup which should be high as you drink tea from a cup, but not for wand and tea as they do not belong together. So, dividing a high probability by a small one should indeed return a relatively high number.\n",
        "\n",
        "- Example 2.1, voldemort hagrid curse - tensor(8.7108)\n",
        "The three words: voldemort, hagrid and curse have a high 𝑃𝑖𝑘/𝑃𝑗𝑘 ratio. This means that curse is very highly related to voldemort but not to hagrid. Voldemort was an evil wizard so he cursed a lot of people and also he himself was cursed. Therefore he occurs very often in the context of the word cursed. Hagrid however was a good person and not a wizard, therefore he was not associated with evil or curses. Therefore the ratio is expected to be high as hagrid - j has a lower probability of occuring with curse - k, than voldemort - i. So the ratio 𝑃𝑖𝑘/𝑃𝑗𝑘 is high.\n",
        "\n",
        "- Example 2.2, voldemort hagrid beast - tensor(0.5588)\n",
        "The three words: voldemort, hagrid and beast have a low 𝑃𝑖𝑘/𝑃𝑗𝑘 ratio which means that the word beast is associated more strongly with hagrid than with voldemort (k occurs more often in combination with j than with i). This is because hagrid had a lot of beasts in his possession and voldemort did not. Therefore the ratio is expected to be low as hagrid - j has a higher probability of occuring with beast - k, than voldemort - i.\n",
        "\n",
        "- For example 3, both outcomes are again as expected. Snape teaches the potion mastery class and so his relation with potion is high but his relation with transfiguration is low as he does not teach this subject. As Mcgonagall teaches the transfiguration class this relation should again be high, but his relation with potions is low as he does not teahc the subject. Thus, for the first result we are dividing a small porbability with a big one which results in a small number and for the second result we are dividing a big probability with a small one which should result in a larger number. This is inline with the results from the function.\n",
        "\n",
        "- Example 4\n",
        "Hedwig is the owl from Harry, so hedwig and owl: high cooccurence\n",
        "scabbers isn't an owl: low cooccurence\n",
        "so the value hedwig scabbers owl (high divided by low) : 5.6266 is high as expected\n",
        "Hedwig isn't a rat, so hedwig and rat: low cooccurence\n",
        "Scabbers is the rat from ron, so scabbers and rat: high occurence so the value hedwig scabbers rat (low divided by high) : 0.0178 is low as expected\n",
        "\n",
        "- Example 5\n",
        "Ron doesn't like to learn and read books, so it has a small value,\n",
        "but Hermione is smart and read books, so a high value.\n",
        "a small value divided by a high value is a small value: 0.6783 low as expected\n",
        "Ron has red hair, so ofcourse it appears often with the word red.\n",
        "and Hermione doesn't have red hair so that's a small cooccurence.\n",
        "a big value divided by a small value is a big value: 2.0282 high as expected.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7hjgLtQsUDo"
      },
      "source": [
        "What would happen if we tried probing out-of-domain words? Use the words that the authors report in the paper as discriminative for \"ice\" and \"steam\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQRnuhpxsUDo",
        "outputId": "56402ce0-0e2b-4878-9d3c-fa984ac7e852"
      },
      "source": [
        "word1 = \"solid\"\n",
        "word2 = \"gas\"\n",
        "print(\"ice\", \"steam\", word1, probe(\"ice\", \"steam\", word1, vocab, P))\n",
        "print(\"ice\", \"steam\", word2, probe(\"ice\", \"steam\", word2, vocab, P))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ice steam solid 0.08204090767103563\n",
            "ice steam gas 0.9024500422545743\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iuqm-km-sUDo"
      },
      "source": [
        "**Interpretation 2**: Give an interpretation of the results you got. Do they match what the authors report in the paper? Why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3KXuakpKzVN"
      },
      "source": [
        "The first result above is the ratio in the relation between ice / solid and steam / solid. The ratio of this relation is 0.082, this shows us that the relation of steam / solid is bigger and that the relation of ice / solid is smaller since steam / solid is the second element of the division. This is unexpected as one would assume that ice would have a stronger relation with solid than steam. The results from the Harry Potter corpus are not in line with the results in the paper. For example. In the paper the relation of ice and solid is big and the relation of solid and steam is relatively small as the resulting ratio is 8.9. This can be explained by the difference in corpus. The paper uses a 6 billion token corpus where as our function only uses the context of Harry Potter where it is unlikely that these word combinations are frequently used.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qLxcPxRsUDo"
      },
      "source": [
        "## Dense Vectors\n",
        "\n",
        "Now, we would like to convert these long sparse vectors into short dense ones. \n",
        "\n",
        "The conversion should be such that the probability ratios we inspected earlier may still be reconstructed via some (for now, unknown) operation $F$ on the dense vectors.\n",
        "\n",
        "To restrict the search space over potential functions, the authors impose a number of constraints they think $F$ should satisfy:\n",
        "1. > While $F$ could be taken to be a complicated function parameterized by, e.g., a neural network, doing so would obfuscate the linear structure we are trying to capture. $F$ should be dot-product based.\n",
        "2. > The distinction between a word and a context word is arbitrary and we are free to exchange the two roles. To do so consistently, we must not only exchange $w \\leftrightarrow \\tilde{w}$ but also $X \\leftrightarrow X^T$.\n",
        "3. > It should be well-defined for all values in $X$.\n",
        "\n",
        "Given these three constraints, each word $i$ in our vocabulary is represented by four vectors:\n",
        "1. A vector $w_i \\in \\mathbb{R}^D$\n",
        "2. A bias $b_i \\in \\mathbb{R}$\n",
        "3. A context vector $\\tilde{w}_i \\in \\mathbb{R}^D$\n",
        "4. A context bias $\\tilde{b}_i \\in \\mathbb{R}$\n",
        "\n",
        "and $F: \\mathbb{R}^D \\times \\mathbb{R} \\times \\mathbb{R}^D \\times \\mathbb{R} \\to \\mathbb{R}$ is defined as:\n",
        "\n",
        "$F(w_i, \\tilde{w}_k, b_i, \\tilde{b}_k) = w_i^T\\tilde{w}_k + b_i + \\tilde{b}_k$.\n",
        "\n",
        "Or equivalently the least squares error $J$ is minimized, where:\n",
        "\n",
        "$J = \\sum_{i,j=1}^{V} f(X_{ij})(w_{i}^T\\tilde{w}_j + b_i + \\tilde{b}_j - log(X_{ij}))^2$\n",
        "\n",
        "with $f$ being a weighting function, defined as \n",
        "\n",
        "$f: \\mathbb{R} \\to \\mathbb{R} = \\begin{cases}\n",
        "    (x/x_{max})^\\alpha, & \\text{if $x<x_{max}$}\\\\\n",
        "    1, & \\text{otherwise}.\n",
        "  \\end{cases}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cxSSUncsUDo"
      },
      "source": [
        "### Weighting Function\n",
        "\n",
        "Let's start with the last part. \n",
        "\n",
        "**Coding 4**: Complete the weighting function `weight_fn` which accepts a co-occurrence matrix ${X}$, a maximum value $x_{max}$ and a fractional power $alpha$, and returns the weighted co-occurrence matrix $f({X})$.\n",
        "\n",
        "Then, compute $\\text{X_weighted}$, the matrix ${X}$ after weighting, using the paper's suggested parameters. \n",
        "\n",
        "_Hint_: Note that $f$ is defined pointwise, so our weighting function should also be pointwise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtME_2mCsUDp",
        "outputId": "c12bace8-0cb2-4ca6-d6d5-ec2344279e2b"
      },
      "source": [
        "def weight_fn(X: FloatTensor, x_max: int, alpha: float) -> FloatTensor:\n",
        "  Ones = torch.ones_like(X)\n",
        "  Alpha = torch.pow(X/x_max, alpha)\n",
        "  return torch.where(torch.greater_equal(X, x_max), Ones, Alpha)\n",
        "\n",
        "X_weighted = weight_fn(X, 100, 3/4)\n",
        "print(X_weighted)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0056, 0.0056, 0.0056,  ..., 0.0056, 0.0056, 0.0056],\n",
            "        [0.0056, 0.0056, 0.0056,  ..., 0.0056, 0.0056, 0.0056],\n",
            "        [0.0056, 0.0056, 0.0056,  ..., 0.0056, 0.0056, 0.0056],\n",
            "        ...,\n",
            "        [0.0056, 0.0056, 0.0056,  ..., 0.0056, 0.0056, 0.0056],\n",
            "        [0.0056, 0.0056, 0.0056,  ..., 0.0056, 0.0056, 0.0056],\n",
            "        [0.0056, 0.0056, 0.0056,  ..., 0.0056, 0.0056, 0.0056]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXiiOEo0T-da"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zvn6KmbSsUDp"
      },
      "source": [
        "Try to get an understanding of how the weighting affects different co-occurrence values (high and low). Think of some word pairs with high and low co-occurrence and look them up in $X$ and in $\\text{X_weighted}$ to get a better idea. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "I0RpN48XsUDp",
        "outputId": "dc94e4ff-365b-4bf1-b78f-dbe8123e812d"
      },
      "source": [
        "word_i = \"voldemort\"\n",
        "word_j = \"beast\"\n",
        "i = vocab[word_i]\n",
        "j = vocab[word_j]\n",
        "\n",
        "print(X[i,j])\n",
        "print(X_weighted[i,j])\n",
        "\n",
        "\"It makes the scale from 0 to 1 instead of very high numbers\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.1000)\n",
            "tensor(0.0552)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'It makes the scale from 0 to 1 instead of very high numbers'"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GthWnISosUDp"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>Show the completed code to your teacher before proceeding</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwFVdQ9MsUDp"
      },
      "source": [
        "### Loss Function\n",
        "\n",
        "Next step is to write the loss function. \n",
        "\n",
        "We can write it as a pointwise function, apply it iteratively over each pair of words and then sum the result; that's however extremely inefficient. \n",
        "\n",
        "Inspecting the formulation of $J$, it is fairly straight-forward to see that it can be immediately implemented using matrix-matrix operations, as:\n",
        "\n",
        "$J = \\sum_{i,j=1}^{V}f(\\mathbf{X})\\cdot(W\\tilde{W}^T + b + \\tilde{b}^T - log(X))^2$,\n",
        "\n",
        "where $W$, $\\tilde{W}$ are the $N \\times D$ matrices containing the $D$-dimensional vectors of all our $N$ vocabulary words, and $b$, $\\tilde{b}$ are the $N \\times 1$ matrices containing the $1$-dimensional biases of our words.\n",
        "\n",
        "**Coding 5**: Complete `loss_fn`, a function that accepts a weighted co-occurrence matrix $f({X})$, the word vectors and biases $W$, $\\tilde{W}$, $b$, $\\tilde{b}$ and the co-occurrence matrix ${X}$, and computes $J$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "538kfC-usUDp"
      },
      "source": [
        "def loss_fn(\n",
        "    X_weighted: FloatTensor, \n",
        "    W: FloatTensor, \n",
        "    W_context: FloatTensor, \n",
        "    B: FloatTensor, \n",
        "    B_context: FloatTensor, \n",
        "    X: FloatTensor\n",
        ") -> FloatTensor:\n",
        "    WWt = W @ W_context.transpose(1,0)\n",
        "    Right = WWt + B + B_context.transpose(1,0) - X.log()\n",
        "    Right = Right.pow(2)\n",
        "    return torch.sum(X_weighted * Right)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4H3W0Gt7sUDp"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>Show the completed code to your teacher before proceeding</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amKlDutZsUDp"
      },
      "source": [
        "### GloVe\n",
        "\n",
        "We have the normalized co-occurrence matrix ${X}$, the weighting function $f$, and the loss function $J$ that implements $F$.\n",
        "\n",
        "What we need now is a mapping from words (or word ids) to unique, parametric and trainable vectors. \n",
        "\n",
        "Torch provides this abstraction in the form of [Embedding layers](https://pytorch.org/docs/stable/nn.html#embedding). Each such layer may be viewed as a stand-alone network that can be optimized using the standard procedure we have already seen. \n",
        "\n",
        "We will utilize the `nn.Module` class to contain all our embedding layers and streamline their joint optimization.\n",
        "The container class will be responsible for a few things:\n",
        "\n",
        "1. **Coding 6.1**: Wrapping the embedding layers:\n",
        "    1. A vector embedding that maps words to $w \\in \\mathbb{R}^D$\n",
        "    2. A context vector embedding that maps words to $w_c \\in \\mathbb{R}^D$\n",
        "    3. A bias embedding that maps words to $b \\in \\mathbb{R}^1$\n",
        "    4. A context bias embedding that maps words to $b_c \\in \\mathbb{R}^1$\n",
        "2. **Coding 6.2**: Implementing `forward`, a function that accepts a weighted co-occurrence matrix $f(X)$, the co-occurrence matrix $X$, then finds the embeddings of all words and finally calls `loss_fn` as defined above.\n",
        "3. **Coding 7**: Implementing `get_vectors`, a function that receives no input and produces the word vectors and context word vectors of all words, adds them together and returns the result, in accordance with the paper:\n",
        "> ...With this in mind, we choose to use the sum $W + \\tilde{W}$ as our word vectors.\n",
        "\n",
        "Complete the network class following the above specifications.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmK-PJBEsUDq"
      },
      "source": [
        "class GloVe(torch.nn.Module):\n",
        "    def __init__(self, vocab: Dict[str, int], vector_dim: int=30, device: str=\"cpu\") -> None:\n",
        "        super(GloVe, self).__init__()\n",
        "        self.device = device\n",
        "        self.vocab_len = len(vocab)\n",
        "        self.w = torch.nn.Embedding(self.vocab_len, vector_dim).to(self.device)\n",
        "        self.wc = torch.nn.Embedding(self.vocab_len, vector_dim).to(self.device)\n",
        "        self.b = torch.nn.Embedding(self.vocab_len, 1).to(self.device)\n",
        "        self.bc = torch.nn.Embedding(self.vocab_len, 1).to(self.device)\n",
        "        \n",
        "    def forward(self, X_weighted: FloatTensor, X: FloatTensor) -> FloatTensor:\n",
        "        embedding_input = torch.arange(self.vocab_len).to(self.device)\n",
        "        W = self.w(embedding_input)\n",
        "        Wc = self.wc(embedding_input)\n",
        "        B = self.b(embedding_input)\n",
        "        Bc = self.bc(embedding_input)\n",
        "        return loss_fn(X_weighted, W, Wc, B, Bc, X)\n",
        "    \n",
        "    def get_vectors(self) -> FloatTensor:\n",
        "        embedding_input = torch.arange(self.vocab_len).to(self.device)\n",
        "        W = self.w(embedding_input)\n",
        "        Wc = self.wc(embedding_input)\n",
        "        return W + Wc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4uPaIspsUDq"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>Show the completed code to your teacher before proceeding</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CK9FKIaZsUDq"
      },
      "source": [
        "### Training\n",
        "\n",
        "Everything is in place; now we may begin optimizing our embedding layers (and in doing so, the vectors they assign). \n",
        "\n",
        "**Coding 8.1**: Instantiate the network class you just defined using $D = 30$. Then instantiate an `Adam` optimizer with a learning rate of 0.05 and train your network for 300 epochs.\n",
        "\n",
        "When writing the training script, remember that your network's forward pass is __already__ computing the loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsF-3FvvsUDq"
      },
      "source": [
        "network = GloVe(vocab=vocab, vector_dim=30)\n",
        "opt = torch.optim.Adam(network.parameters(), lr=0.05)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jt1I2ukZsUDq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd29f62d-e495-45b8-b43c-e9d461646d61"
      },
      "source": [
        "num_epochs = 300\n",
        "losses = []\n",
        "for i in range(num_epochs):\n",
        "    loss = network.forward(X_weighted, X) # loss computation (optionally print it out)\n",
        "    print(\"Epoch {}\".format(i))\n",
        "    print(\" Loss: {}\".format(loss))\n",
        "    gradient = network.get_vectors() # gradient computation\n",
        "    loss.backward() # back-propagation\n",
        "    opt.step()\n",
        "    opt.zero_grad() # gradient reset\n",
        "    losses.append(loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n",
            " Loss: 11867773.0\n",
            "Epoch 1\n",
            " Loss: 10298086.0\n",
            "Epoch 2\n",
            " Loss: 8940909.0\n",
            "Epoch 3\n",
            " Loss: 7773023.0\n",
            "Epoch 4\n",
            " Loss: 6771115.5\n",
            "Epoch 5\n",
            " Loss: 5913496.5\n",
            "Epoch 6\n",
            " Loss: 5180439.0\n",
            "Epoch 7\n",
            " Loss: 4554211.0\n",
            "Epoch 8\n",
            " Loss: 4019053.75\n",
            "Epoch 9\n",
            " Loss: 3561130.0\n",
            "Epoch 10\n",
            " Loss: 3168426.25\n",
            "Epoch 11\n",
            " Loss: 2830609.25\n",
            "Epoch 12\n",
            " Loss: 2538856.75\n",
            "Epoch 13\n",
            " Loss: 2285686.0\n",
            "Epoch 14\n",
            " Loss: 2064787.5\n",
            "Epoch 15\n",
            " Loss: 1870880.625\n",
            "Epoch 16\n",
            " Loss: 1699590.0\n",
            "Epoch 17\n",
            " Loss: 1547336.625\n",
            "Epoch 18\n",
            " Loss: 1411247.375\n",
            "Epoch 19\n",
            " Loss: 1289073.375\n",
            "Epoch 20\n",
            " Loss: 1179108.75\n",
            "Epoch 21\n",
            " Loss: 1080103.5\n",
            "Epoch 22\n",
            " Loss: 991162.9375\n",
            "Epoch 23\n",
            " Loss: 911634.5625\n",
            "Epoch 24\n",
            " Loss: 840991.25\n",
            "Epoch 25\n",
            " Loss: 778726.875\n",
            "Epoch 26\n",
            " Loss: 724281.125\n",
            "Epoch 27\n",
            " Loss: 677006.3125\n",
            "Epoch 28\n",
            " Loss: 636173.6875\n",
            "Epoch 29\n",
            " Loss: 601004.0\n",
            "Epoch 30\n",
            " Loss: 570710.375\n",
            "Epoch 31\n",
            " Loss: 544541.1875\n",
            "Epoch 32\n",
            " Loss: 521813.09375\n",
            "Epoch 33\n",
            " Loss: 501933.4375\n",
            "Epoch 34\n",
            " Loss: 484410.1875\n",
            "Epoch 35\n",
            " Loss: 468850.1875\n",
            "Epoch 36\n",
            " Loss: 454949.28125\n",
            "Epoch 37\n",
            " Loss: 442475.09375\n",
            "Epoch 38\n",
            " Loss: 431248.75\n",
            "Epoch 39\n",
            " Loss: 421127.46875\n",
            "Epoch 40\n",
            " Loss: 411991.53125\n",
            "Epoch 41\n",
            " Loss: 403736.125\n",
            "Epoch 42\n",
            " Loss: 396267.46875\n",
            "Epoch 43\n",
            " Loss: 389500.875\n",
            "Epoch 44\n",
            " Loss: 383360.59375\n",
            "Epoch 45\n",
            " Loss: 377778.8125\n",
            "Epoch 46\n",
            " Loss: 372695.21875\n",
            "Epoch 47\n",
            " Loss: 368055.8125\n",
            "Epoch 48\n",
            " Loss: 363812.6875\n",
            "Epoch 49\n",
            " Loss: 359923.09375\n",
            "Epoch 50\n",
            " Loss: 356349.25\n",
            "Epoch 51\n",
            " Loss: 353057.9375\n",
            "Epoch 52\n",
            " Loss: 350020.03125\n",
            "Epoch 53\n",
            " Loss: 347210.4375\n",
            "Epoch 54\n",
            " Loss: 344607.375\n",
            "Epoch 55\n",
            " Loss: 342192.0\n",
            "Epoch 56\n",
            " Loss: 339947.875\n",
            "Epoch 57\n",
            " Loss: 337860.0625\n",
            "Epoch 58\n",
            " Loss: 335915.0625\n",
            "Epoch 59\n",
            " Loss: 334100.0625\n",
            "Epoch 60\n",
            " Loss: 332403.09375\n",
            "Epoch 61\n",
            " Loss: 330813.09375\n",
            "Epoch 62\n",
            " Loss: 329319.625\n",
            "Epoch 63\n",
            " Loss: 327913.25\n",
            "Epoch 64\n",
            " Loss: 326585.34375\n",
            "Epoch 65\n",
            " Loss: 325327.5\n",
            "Epoch 66\n",
            " Loss: 324132.09375\n",
            "Epoch 67\n",
            " Loss: 322991.78125\n",
            "Epoch 68\n",
            " Loss: 321899.78125\n",
            "Epoch 69\n",
            " Loss: 320849.90625\n",
            "Epoch 70\n",
            " Loss: 319836.65625\n",
            "Epoch 71\n",
            " Loss: 318855.46875\n",
            "Epoch 72\n",
            " Loss: 317902.71875\n",
            "Epoch 73\n",
            " Loss: 316975.4375\n",
            "Epoch 74\n",
            " Loss: 316071.59375\n",
            "Epoch 75\n",
            " Loss: 315189.4375\n",
            "Epoch 76\n",
            " Loss: 314327.5\n",
            "Epoch 77\n",
            " Loss: 313484.59375\n",
            "Epoch 78\n",
            " Loss: 312659.3125\n",
            "Epoch 79\n",
            " Loss: 311850.5\n",
            "Epoch 80\n",
            " Loss: 311056.9375\n",
            "Epoch 81\n",
            " Loss: 310277.625\n",
            "Epoch 82\n",
            " Loss: 309511.59375\n",
            "Epoch 83\n",
            " Loss: 308758.03125\n",
            "Epoch 84\n",
            " Loss: 308016.21875\n",
            "Epoch 85\n",
            " Loss: 307285.625\n",
            "Epoch 86\n",
            " Loss: 306565.9375\n",
            "Epoch 87\n",
            " Loss: 305856.78125\n",
            "Epoch 88\n",
            " Loss: 305158.125\n",
            "Epoch 89\n",
            " Loss: 304469.96875\n",
            "Epoch 90\n",
            " Loss: 303792.375\n",
            "Epoch 91\n",
            " Loss: 303125.4375\n",
            "Epoch 92\n",
            " Loss: 302469.375\n",
            "Epoch 93\n",
            " Loss: 301824.125\n",
            "Epoch 94\n",
            " Loss: 301189.875\n",
            "Epoch 95\n",
            " Loss: 300566.5\n",
            "Epoch 96\n",
            " Loss: 299954.09375\n",
            "Epoch 97\n",
            " Loss: 299352.5\n",
            "Epoch 98\n",
            " Loss: 298761.59375\n",
            "Epoch 99\n",
            " Loss: 298181.25\n",
            "Epoch 100\n",
            " Loss: 297611.3125\n",
            "Epoch 101\n",
            " Loss: 297051.6875\n",
            "Epoch 102\n",
            " Loss: 296502.125\n",
            "Epoch 103\n",
            " Loss: 295962.5\n",
            "Epoch 104\n",
            " Loss: 295432.59375\n",
            "Epoch 105\n",
            " Loss: 294912.28125\n",
            "Epoch 106\n",
            " Loss: 294401.34375\n",
            "Epoch 107\n",
            " Loss: 293899.5625\n",
            "Epoch 108\n",
            " Loss: 293406.8125\n",
            "Epoch 109\n",
            " Loss: 292922.90625\n",
            "Epoch 110\n",
            " Loss: 292447.6875\n",
            "Epoch 111\n",
            " Loss: 291981.03125\n",
            "Epoch 112\n",
            " Loss: 291522.8125\n",
            "Epoch 113\n",
            " Loss: 291072.84375\n",
            "Epoch 114\n",
            " Loss: 290631.09375\n",
            "Epoch 115\n",
            " Loss: 290197.4375\n",
            "Epoch 116\n",
            " Loss: 289771.71875\n",
            "Epoch 117\n",
            " Loss: 289353.84375\n",
            "Epoch 118\n",
            " Loss: 288943.75\n",
            "Epoch 119\n",
            " Loss: 288541.34375\n",
            "Epoch 120\n",
            " Loss: 288146.46875\n",
            "Epoch 121\n",
            " Loss: 287759.09375\n",
            "Epoch 122\n",
            " Loss: 287379.0625\n",
            "Epoch 123\n",
            " Loss: 287006.25\n",
            "Epoch 124\n",
            " Loss: 286640.71875\n",
            "Epoch 125\n",
            " Loss: 286282.21875\n",
            "Epoch 126\n",
            " Loss: 285930.75\n",
            "Epoch 127\n",
            " Loss: 285586.09375\n",
            "Epoch 128\n",
            " Loss: 285248.1875\n",
            "Epoch 129\n",
            " Loss: 284916.9375\n",
            "Epoch 130\n",
            " Loss: 284592.1875\n",
            "Epoch 131\n",
            " Loss: 284273.75\n",
            "Epoch 132\n",
            " Loss: 283961.65625\n",
            "Epoch 133\n",
            " Loss: 283655.625\n",
            "Epoch 134\n",
            " Loss: 283355.53125\n",
            "Epoch 135\n",
            " Loss: 283061.375\n",
            "Epoch 136\n",
            " Loss: 282772.875\n",
            "Epoch 137\n",
            " Loss: 282489.9375\n",
            "Epoch 138\n",
            " Loss: 282212.40625\n",
            "Epoch 139\n",
            " Loss: 281940.1875\n",
            "Epoch 140\n",
            " Loss: 281673.09375\n",
            "Epoch 141\n",
            " Loss: 281411.03125\n",
            "Epoch 142\n",
            " Loss: 281153.78125\n",
            "Epoch 143\n",
            " Loss: 280901.25\n",
            "Epoch 144\n",
            " Loss: 280653.25\n",
            "Epoch 145\n",
            " Loss: 280409.75\n",
            "Epoch 146\n",
            " Loss: 280170.53125\n",
            "Epoch 147\n",
            " Loss: 279935.46875\n",
            "Epoch 148\n",
            " Loss: 279704.46875\n",
            "Epoch 149\n",
            " Loss: 279477.375\n",
            "Epoch 150\n",
            " Loss: 279254.0625\n",
            "Epoch 151\n",
            " Loss: 279034.40625\n",
            "Epoch 152\n",
            " Loss: 278818.34375\n",
            "Epoch 153\n",
            " Loss: 278605.6875\n",
            "Epoch 154\n",
            " Loss: 278396.375\n",
            "Epoch 155\n",
            " Loss: 278190.28125\n",
            "Epoch 156\n",
            " Loss: 277987.3125\n",
            "Epoch 157\n",
            " Loss: 277787.3125\n",
            "Epoch 158\n",
            " Loss: 277590.3125\n",
            "Epoch 159\n",
            " Loss: 277396.09375\n",
            "Epoch 160\n",
            " Loss: 277204.65625\n",
            "Epoch 161\n",
            " Loss: 277015.875\n",
            "Epoch 162\n",
            " Loss: 276829.71875\n",
            "Epoch 163\n",
            " Loss: 276646.03125\n",
            "Epoch 164\n",
            " Loss: 276464.78125\n",
            "Epoch 165\n",
            " Loss: 276285.9375\n",
            "Epoch 166\n",
            " Loss: 276109.40625\n",
            "Epoch 167\n",
            " Loss: 275935.0625\n",
            "Epoch 168\n",
            " Loss: 275762.9375\n",
            "Epoch 169\n",
            " Loss: 275592.9375\n",
            "Epoch 170\n",
            " Loss: 275425.0\n",
            "Epoch 171\n",
            " Loss: 275259.0625\n",
            "Epoch 172\n",
            " Loss: 275095.125\n",
            "Epoch 173\n",
            " Loss: 274933.0625\n",
            "Epoch 174\n",
            " Loss: 274772.9375\n",
            "Epoch 175\n",
            " Loss: 274614.625\n",
            "Epoch 176\n",
            " Loss: 274458.125\n",
            "Epoch 177\n",
            " Loss: 274303.34375\n",
            "Epoch 178\n",
            " Loss: 274150.34375\n",
            "Epoch 179\n",
            " Loss: 273999.0\n",
            "Epoch 180\n",
            " Loss: 273849.375\n",
            "Epoch 181\n",
            " Loss: 273701.34375\n",
            "Epoch 182\n",
            " Loss: 273554.90625\n",
            "Epoch 183\n",
            " Loss: 273410.0625\n",
            "Epoch 184\n",
            " Loss: 273266.78125\n",
            "Epoch 185\n",
            " Loss: 273125.0\n",
            "Epoch 186\n",
            " Loss: 272984.75\n",
            "Epoch 187\n",
            " Loss: 272845.96875\n",
            "Epoch 188\n",
            " Loss: 272708.65625\n",
            "Epoch 189\n",
            " Loss: 272572.8125\n",
            "Epoch 190\n",
            " Loss: 272438.40625\n",
            "Epoch 191\n",
            " Loss: 272305.375\n",
            "Epoch 192\n",
            " Loss: 272173.75\n",
            "Epoch 193\n",
            " Loss: 272043.53125\n",
            "Epoch 194\n",
            " Loss: 271914.59375\n",
            "Epoch 195\n",
            " Loss: 271787.03125\n",
            "Epoch 196\n",
            " Loss: 271660.84375\n",
            "Epoch 197\n",
            " Loss: 271535.9375\n",
            "Epoch 198\n",
            " Loss: 271412.3125\n",
            "Epoch 199\n",
            " Loss: 271289.96875\n",
            "Epoch 200\n",
            " Loss: 271168.90625\n",
            "Epoch 201\n",
            " Loss: 271049.125\n",
            "Epoch 202\n",
            " Loss: 270930.5625\n",
            "Epoch 203\n",
            " Loss: 270813.21875\n",
            "Epoch 204\n",
            " Loss: 270697.09375\n",
            "Epoch 205\n",
            " Loss: 270582.21875\n",
            "Epoch 206\n",
            " Loss: 270468.46875\n",
            "Epoch 207\n",
            " Loss: 270355.9375\n",
            "Epoch 208\n",
            " Loss: 270244.53125\n",
            "Epoch 209\n",
            " Loss: 270134.28125\n",
            "Epoch 210\n",
            " Loss: 270025.21875\n",
            "Epoch 211\n",
            " Loss: 269917.21875\n",
            "Epoch 212\n",
            " Loss: 269810.375\n",
            "Epoch 213\n",
            " Loss: 269704.59375\n",
            "Epoch 214\n",
            " Loss: 269599.90625\n",
            "Epoch 215\n",
            " Loss: 269496.28125\n",
            "Epoch 216\n",
            " Loss: 269393.6875\n",
            "Epoch 217\n",
            " Loss: 269292.1875\n",
            "Epoch 218\n",
            " Loss: 269191.65625\n",
            "Epoch 219\n",
            " Loss: 269092.1875\n",
            "Epoch 220\n",
            " Loss: 268993.71875\n",
            "Epoch 221\n",
            " Loss: 268896.25\n",
            "Epoch 222\n",
            " Loss: 268799.71875\n",
            "Epoch 223\n",
            " Loss: 268704.1875\n",
            "Epoch 224\n",
            " Loss: 268609.5625\n",
            "Epoch 225\n",
            " Loss: 268515.90625\n",
            "Epoch 226\n",
            " Loss: 268423.15625\n",
            "Epoch 227\n",
            " Loss: 268331.34375\n",
            "Epoch 228\n",
            " Loss: 268240.40625\n",
            "Epoch 229\n",
            " Loss: 268150.375\n",
            "Epoch 230\n",
            " Loss: 268061.1875\n",
            "Epoch 231\n",
            " Loss: 267972.875\n",
            "Epoch 232\n",
            " Loss: 267885.40625\n",
            "Epoch 233\n",
            " Loss: 267798.75\n",
            "Epoch 234\n",
            " Loss: 267712.9375\n",
            "Epoch 235\n",
            " Loss: 267627.90625\n",
            "Epoch 236\n",
            " Loss: 267543.6875\n",
            "Epoch 237\n",
            " Loss: 267460.25\n",
            "Epoch 238\n",
            " Loss: 267377.625\n",
            "Epoch 239\n",
            " Loss: 267295.6875\n",
            "Epoch 240\n",
            " Loss: 267214.5625\n",
            "Epoch 241\n",
            " Loss: 267134.15625\n",
            "Epoch 242\n",
            " Loss: 267054.5\n",
            "Epoch 243\n",
            " Loss: 266975.5\n",
            "Epoch 244\n",
            " Loss: 266897.25\n",
            "Epoch 245\n",
            " Loss: 266819.6875\n",
            "Epoch 246\n",
            " Loss: 266742.8125\n",
            "Epoch 247\n",
            " Loss: 266666.59375\n",
            "Epoch 248\n",
            " Loss: 266591.0625\n",
            "Epoch 249\n",
            " Loss: 266516.125\n",
            "Epoch 250\n",
            " Loss: 266441.90625\n",
            "Epoch 251\n",
            " Loss: 266368.28125\n",
            "Epoch 252\n",
            " Loss: 266295.25\n",
            "Epoch 253\n",
            " Loss: 266222.875\n",
            "Epoch 254\n",
            " Loss: 266151.0625\n",
            "Epoch 255\n",
            " Loss: 266079.875\n",
            "Epoch 256\n",
            " Loss: 266009.28125\n",
            "Epoch 257\n",
            " Loss: 265939.28125\n",
            "Epoch 258\n",
            " Loss: 265869.78125\n",
            "Epoch 259\n",
            " Loss: 265800.875\n",
            "Epoch 260\n",
            " Loss: 265732.5625\n",
            "Epoch 261\n",
            " Loss: 265664.71875\n",
            "Epoch 262\n",
            " Loss: 265597.4375\n",
            "Epoch 263\n",
            " Loss: 265530.6875\n",
            "Epoch 264\n",
            " Loss: 265464.46875\n",
            "Epoch 265\n",
            " Loss: 265398.75\n",
            "Epoch 266\n",
            " Loss: 265333.53125\n",
            "Epoch 267\n",
            " Loss: 265268.8125\n",
            "Epoch 268\n",
            " Loss: 265204.5625\n",
            "Epoch 269\n",
            " Loss: 265140.8125\n",
            "Epoch 270\n",
            " Loss: 265077.53125\n",
            "Epoch 271\n",
            " Loss: 265014.75\n",
            "Epoch 272\n",
            " Loss: 264952.375\n",
            "Epoch 273\n",
            " Loss: 264890.5\n",
            "Epoch 274\n",
            " Loss: 264829.03125\n",
            "Epoch 275\n",
            " Loss: 264768.03125\n",
            "Epoch 276\n",
            " Loss: 264707.46875\n",
            "Epoch 277\n",
            " Loss: 264647.3125\n",
            "Epoch 278\n",
            " Loss: 264587.59375\n",
            "Epoch 279\n",
            " Loss: 264528.28125\n",
            "Epoch 280\n",
            " Loss: 264469.375\n",
            "Epoch 281\n",
            " Loss: 264410.90625\n",
            "Epoch 282\n",
            " Loss: 264352.8125\n",
            "Epoch 283\n",
            " Loss: 264295.09375\n",
            "Epoch 284\n",
            " Loss: 264237.8125\n",
            "Epoch 285\n",
            " Loss: 264180.90625\n",
            "Epoch 286\n",
            " Loss: 264124.34375\n",
            "Epoch 287\n",
            " Loss: 264068.1875\n",
            "Epoch 288\n",
            " Loss: 264012.375\n",
            "Epoch 289\n",
            " Loss: 263956.9375\n",
            "Epoch 290\n",
            " Loss: 263901.875\n",
            "Epoch 291\n",
            " Loss: 263847.09375\n",
            "Epoch 292\n",
            " Loss: 263792.71875\n",
            "Epoch 293\n",
            " Loss: 263738.6875\n",
            "Epoch 294\n",
            " Loss: 263685.0\n",
            "Epoch 295\n",
            " Loss: 263631.625\n",
            "Epoch 296\n",
            " Loss: 263578.59375\n",
            "Epoch 297\n",
            " Loss: 263525.90625\n",
            "Epoch 298\n",
            " Loss: 263473.5\n",
            "Epoch 299\n",
            " Loss: 263421.4375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhmDRf0ssUDq"
      },
      "source": [
        "**Coding 8.2**: Plot the losses and examine the learning curve. Is its shape what you would expect it to be?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r1HIEmlsUDq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "outputId": "81c1b475-ecce-478b-f07b-6f2247cbdbdc"
      },
      "source": [
        "# Your plotting here\n",
        "from matplotlib import pyplot as plt\n",
        "plt.plot(losses)\n",
        "plt.legend([\"Training\"])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEh42P4osUDr"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>Show the completed code to your teacher before proceeding</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu0_7wWdsUDr"
      },
      "source": [
        "### Validation (Similarity)\n",
        "\n",
        "Curious to see what this network has learned? Let's perform a simple validation experiment. \n",
        "\n",
        "We will check which words the models considers the most similar to other words. To that end, we need a notion of __similarity__. One of the most common measures of similarity in high dimensional vector spaces is the cosine similarity. \n",
        "\n",
        "The cosine similarity of two vectors $\\vec{a}, \\vec{b}$ is given as:\n",
        "$$sim(\\vec{a}, \\vec{b}) = \\frac{\\vec{a}\\cdot \\vec{b}}{|\\vec{a}|_2 \\cdot |\\vec{b}|_2}$$\n",
        "\n",
        "where $|\\vec{x}|_2$ is the $L_2$-norm of the $\\vec{x}$.\n",
        "\n",
        "The function `similarity` below accepts two words, a vocabulary and the network's output vectors, and computes the similarity between these two words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-Bl2FXisUDr"
      },
      "source": [
        "def similarity(word_i: str, word_j: str, vocab: Dict[str, int], vectors: FloatTensor) -> float:\n",
        "    i = vocab[word_i]\n",
        "    j = vocab[word_j] \n",
        "    v_i = vectors[i] / torch.norm(vectors[i], p=2)  # a/|a|\n",
        "    v_j = vectors[j] / torch.norm(vectors[j], p=2)  # b/|b|\n",
        "    sim = torch.mm(v_i.view(1, -1), v_j.view(-1, 1)).item()\n",
        "    return sim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keDhqHpssUDr"
      },
      "source": [
        "Let's check out some examples. Consider the word pairs below and, optionally, add your own word pairs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPdElHGNsUDr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4770078-8a2f-4c26-c058-40a729342797"
      },
      "source": [
        "word_vectors = network.get_vectors().detach()\n",
        "\n",
        "for pair in [\n",
        "    (\"cruciatus\", \"imperius\"), \n",
        "    (\"avada\", \"kedavra\"), \n",
        "    (\"hogwarts\", \"school\"), \n",
        "    (\"goblin\", \"hagrid\"), \n",
        "    (\"giant\", \"hagrid\"),\n",
        "]:\n",
        "    \n",
        "    print(\"Similarity between '{}' and '{}' is: {}\".\n",
        "          format(pair[0], pair[1], similarity(pair[0], pair[1], vocab, word_vectors)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between 'cruciatus' and 'imperius' is: 0.46702688932418823\n",
            "Similarity between 'avada' and 'kedavra' is: 0.825108528137207\n",
            "Similarity between 'hogwarts' and 'school' is: 0.8492355346679688\n",
            "Similarity between 'goblin' and 'hagrid' is: 0.006633872631937265\n",
            "Similarity between 'giant' and 'hagrid' is: 0.4974243938922882\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZOZqDKSsUDr"
      },
      "source": [
        "**Interpretation 3**: Give a brief interpretation of the results. Do the scores correspond well to your perceived similarity of these word pairs?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZIsxe6okjbV"
      },
      "source": [
        "1) Both cruciatus and imperius are an unforgivable curse in the context of Harry Potter. Because both belong to the same group of curses, it explain why the precieved similarity between the two words is relatively high (0.679) but not too high as they are not \n",
        "\n",
        "2) The similarity between avada and kedavra is high, with a similarity score of 0.72. This means that the words have a higher chance of occuring together than by themselves. It is expected that these words have a higher similarity than 0.72 because the words avada and kedavra together form one spell and, therefore, are almost exclusively used together.\n",
        "\n",
        "3) Hogwarts is the School of Witchcraft and Wizardry. It should be expected that the word Hogwarts and school appear often together. The similarity between Hogwart and school is 0.78, highly related as expected.\n",
        "\n",
        "4) As expected the score for 'goblin' and 'Hagrid' is roughly the same as the perceived similarity as both words are used in different contexts but do have some overlap (the bank scene at the start of book 1).\n",
        "\n",
        "5) The similarity between giant and hagrid is 0.35. This means that these two words do not often occur together. Despite the fact that hagrid is a half-giant, this is not often mentioned in the book and therefore the combination does not occur together very often. However, hagrid is mentioned often and therefore the denominator of the similarity formula is much higher than the nominator and the similarty is low."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7nuzA3_sUDr"
      },
      "source": [
        "To obtain the similarities of one word against all other words in the corpus, we may rewrite the above equation as:\n",
        "$$sim(\\vec{w}, \\mathbf{C}) = \\frac{\\vec{w}\\cdot \\mathbf{C}}{|\\vec{w}|_2 \\cdot |\\mathbf{C}|_2}$$\n",
        "\n",
        "**Coding 9**: Using `similarity` as a reference, write `similarities`, which accepts one word, a vocabulary and the network's output vectors and computes the similarity between the word and the entire corpus.\n",
        "\n",
        "_Hint_: $\\mathbf{C} \\in \\mathbb{R}^{N, D}$, $\\vec{w} \\in \\mathbb{R}^{1, D}$, $sim(\\vec{w}, \\mathbf{C}) \\in \\mathbb{R}^{1, N}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsWcjgSdsnZr"
      },
      "source": [
        "def similarities(word_i: str, vocab: Dict[str, int], vectors: FloatTensor) -> FloatTensor:\n",
        "    i = vocab[word_i]\n",
        "    v_i = vectors[i] / torch.norm(vectors[i], p=2)  # a/|a|\n",
        "    C2 = torch.sqrt(torch.sum(vectors*vectors, dim=1))\n",
        "\n",
        "    left = vectors[i] / torch.norm(vectors[i], p=2)\n",
        "    right = vectors / C2[:,None]\n",
        "\n",
        "    right = torch.transpose(right,1,0)\n",
        "    #sim = torch.mm(v_i.view(1, -1), C_i.view(-1, 1)).item()\n",
        "    return left @ right"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ouOPvgQsUDs"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>Show the completed code to your teacher before proceeding</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdmjgvj-sUDs"
      },
      "source": [
        "Now we can manipulate the word vectors to find out what the corpus-wide most similar words to a query word are!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iINA9OUAsUDs"
      },
      "source": [
        "def most_similar(word_i: str, vocab: Dict[str, int], vectors: FloatTensor, k: int) -> List[str]:\n",
        "    sims = similarities(word_i, vocab, vectors)\n",
        "    _, topi = sims.topk(dim=-1, k=k)\n",
        "    topi = topi.view(-1).cpu().numpy().tolist()\n",
        "    inv = {v: i for i, v in vocab.items()}\n",
        "    return [inv[i] for i in topi if inv[i] != word_i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aj2x8Cc2sUDs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd0222ca-2b4f-4085-a4ff-5f02d80c068c"
      },
      "source": [
        "for word in [\n",
        "    \"forbidden\", \"myrtle\", \"gryffindor\", \"wand\", \"quidditch\", \"marauder\",\n",
        "    \"horcrux\", \"phoenix\", \"triwizard\", \"screaming\", \"letter\"\n",
        "]:\n",
        "    print(\"Most similar words to '{}': {}\".format(word, most_similar(word, vocab, word_vectors, 6)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar words to 'forbidden': ['forest', 'trees', 'castle', 'tree', 'follow']\n",
            "Most similar words to 'myrtle': ['toilet', 'bathroom', 'moaning', 'peeves', 'ignoring']\n",
            "Most similar words to 'gryffindor': ['ravenclaw', 'slytherin', 'match', 'points', 'gryffindors']\n",
            "Most similar words to 'wand': ['drawn', 'inches', 'faster', 'ready', 'harry']\n",
            "Most similar words to 'quidditch': ['match', 'field', 'starting', 'prospect', 'win']\n",
            "Most similar words to 'marauder': ['cupboard', 'tapestry', 'extra', 'map', 'floor']\n",
            "Most similar words to 'horcrux': ['fake', 'sword', 'treasure', 'goblin', 'safe']\n",
            "Most similar words to 'phoenix': ['fawkes', 'feather', 'order', 'connection', 'flames']\n",
            "Most similar words to 'triwizard': ['tournament', 'final', 'tasks', 'champion', 'task']\n",
            "Most similar words to 'screaming': ['directly', 'stopping', 'bellowed', 'vision', 'crash']\n",
            "Most similar words to 'letter': ['envelope', 'hedwig', 'written', 'stuck', 'owl']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juEhlQ5ZsUDs"
      },
      "source": [
        "**Interpretation 4**: Interpret the results.\n",
        "- Do these most similar words make sense (are they actually similar to the query words)? \n",
        "- Are there any patterns you can see in the \"errors\" (the words that you woudn't consider actually similar to the query word)? \n",
        "- Would you say that the model captures similarity, relatedness, both or neither?\n",
        "- Any other observations are welcome.\n",
        "\n",
        "Illustrate your answers with examples from your model's output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDZPe3-L5TCA"
      },
      "source": [
        "1) The most similar words make a lot of sense for a lot of the words. However, for most words they are not really similar to the words in meaning, but more related on a contextual level. For example, the meaning of \"forest\" is not similar to the meaning of \"forbidden\", but they are related in the sense that there is a \"forbidden forest\" in Harry Potter. This hold true for most of the word relations. An example for similarity would be the words that are most similar to \"gryffindor\" as both \"ravenclaw\" and \"slytherin\" are a one of the four houses of Hogwards. However, these words are again not similar in the sense that they represent the exact same thing like \"small\" and \"little\" which are synonyms of eachother, but rather represent the same relation as they are all three a house of Hogwards.\n",
        "\n",
        "2) There are some patterns in the similar words that are not necessarily related to the target word but merely often occur in the same context as the target word. for instance: the target word 'screaming' has nothing to do with 'directly' or 'vision', but still these words have a high similarity because they occur in the same context. In harry potter harry sometimes has visions of people screaming. Also the target word 'horcrux' has nothing semantically to do with 'goblin' or 'safe', but these often occur in the same context as the goblins are keepers of the safe in which a horcrux is kept. Therefore, the similarity is high while the meaning is not very similar.\n",
        "\n",
        "3) The model captures similarity because it's based on context and doesn't actually capture any meaning of the words. For that the model would have to have an internal, manipulable representation of the world equivalent to our systems of concepts. Even if the model would also use grammatical tags, it fails to use whatever we see as 'understanding of the meaning'\n",
        "\n",
        "4) Other observation: while the plurals of words have a high similarity with target words, they are not classified as similar because they don't often occur in the same context. Often either a singular or plural form is used for a word and not both.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_VWbhDJsUDs"
      },
      "source": [
        "Overall it's quite impressive; we managed to encode a meaningful portion of the corpus statistics in only $30$ numbers per word! \n",
        "(A compression ratio of 99.4%)\n",
        "\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "<b>Note:</b> The word vectors obtained by this process are (to a small extent) random, due to the random initialization of the embedding layers. If you are unhappy with your results, you can repeat the experiment a few times or try to toy around with the hyper-parameters (the smoothing factor of ${X}$, $x_{max}$, $\\alpha$, the number of epochs and the dimensionality of the vector space).\n",
        "</div>\n",
        "\n",
        "Word vectors, however, contain way more information than just word co-occurrence statistics. Hold tight until the next assignment, where we will see how word vectors may be used to infer information spanning entire phrases and sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7KOZ0yVsUDs"
      },
      "source": [
        "### Validation (Word Analogies)\n",
        "\n",
        "From the paper:\n",
        "> The word analogy task consists of questions like \"$a$ is to $b$ as is $c$ to $?$\" To correctly answer this question, we must find the word $d$ such that $w_d \\approx w_b - w_a + w_c$ according to the cosine similarity.\n",
        "\n",
        "**Coding 10**: Write your own function that performs the word analogy task.\n",
        "\n",
        "_Hint_: Take a look at the code a few cells back. Most of what you need is already there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdyikfOI8AZx"
      },
      "source": [
        "def analogy(\n",
        "    word_a: str, word_b: str, word_c: str, vocab: Dict[str, int], vectors: FloatTensor, k: int\n",
        ") -> List[str]:\n",
        "    a = vocab[word_a]\n",
        "    b = vocab[word_b]\n",
        "    c = vocab[word_c]\n",
        "\n",
        "    wd = vectors[b] - vectors[a] + vectors[c]\n",
        "\n",
        "    return most_similar_for_analogy(wd, word_c, vocab, vectors, k)\n",
        "    # call function similarity with wd \n",
        "    # wd doorgeven, return highest\n",
        "    # geeft int door\n",
        "\n",
        "\n",
        "def similarities_for_analogy(wd_i: int, vocab: Dict[str, int], vectors: FloatTensor) -> FloatTensor:\n",
        "    v_i = wd_i / torch.norm(wd_i, p=2)  # a/|a|\n",
        "    C2 = torch.sqrt(torch.sum(vectors*vectors, dim=1))\n",
        "    left = wd_i / torch.norm(wd_i, p=2)\n",
        "    right = vectors / C2[:,None]\n",
        "    right = torch.transpose(right,1,0)\n",
        "    #sim = torch.mm(v_i.view(1, -1), C_i.view(-1, 1)).item()\n",
        "    return left @ right\n",
        "\n",
        "def most_similar_for_analogy(wd_i: int, word_c: str, vocab: Dict[str, int], vectors: FloatTensor, k: int) -> List[str]:\n",
        "    sims = similarities_for_analogy(wd_i, vocab, vectors)\n",
        "    _, topi = sims.topk(dim=-1, k=k)\n",
        "    topi = topi.view(-1).cpu().numpy().tolist()\n",
        "    inv = {v: i for i, v in vocab.items()}\n",
        "    return [inv[i] for i in topi if inv[i] != word_c]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIuoGMtq8As6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDNwDUwUsUDt"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>Show the completed code to your teacher before proceeding</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkR6Z2dWsUDt"
      },
      "source": [
        "Some example triplets to test your analogies on:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUBk_f2EsUDt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6aede65-e6cc-4001-891d-a2951c425c59"
      },
      "source": [
        "triplets = [(\"padma\", \"parvati\", \"fred\"),\n",
        "            (\"avada\", \"kedavra\", \"expecto\"),\n",
        "            (\"dungeon\", \"slytherin\", \"tower\"),\n",
        "            (\"scabbers\", \"ron\", \"hedwig\"),\n",
        "            (\"ron\", \"molly\", \"draco\"),\n",
        "            (\"durmstrang\", \"viktor\", \"beauxbatons\"),\n",
        "            (\"snape\", \"potions\", \"trelawney\"),\n",
        "            (\"harry\", \"seeker\", \"ron\")\n",
        "           ]\n",
        "\n",
        "for a, b, c in triplets:\n",
        "    print(\"'{}' is to '{}' as '{}' is to {}\".format(a, b, c, analogy(a, b, c, vocab, word_vectors, 6)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'padma' is to 'parvati' as 'fred' is to ['weasley', 'angrily', 'away', 'm', 've']\n",
            "'avada' is to 'kedavra' as 'expecto' is to ['patronum', 'patronus', 'vapor', 'wand:', 'erupted']\n",
            "'dungeon' is to 'slytherin' as 'tower' is to ['gryffindor', 'slytherin', 'ravenclaw', 'lead', 'quidditch']\n",
            "'scabbers' is to 'ron' as 'hedwig' is to ['harry', 'ron', 'just', 'wondering', 'relieved', 'hermione']\n",
            "'ron' is to 'molly' as 'draco' is to ['curling', 'pleading', 'resentful', 'defensively', 'molly', 'sisters']\n",
            "'durmstrang' is to 'viktor' as 'beauxbatons' is to ['viktor', 'grinned', 'ball', 'davies', 'krum']\n",
            "'snape' is to 'potions' as 'trelawney' is to ['potions', 'subject', 'herbology', 'sixth', 'disgruntled']\n",
            "'harry' is to 'seeker' as 'ron' is to ['captain', 'oliver', 'match', 'seeker', 'mud', 'quidditch']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uphdYcksUDt"
      },
      "source": [
        "Some minimal emergent intelligence :) *(hopefully..)*. 🧙‍♀️"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4M3zufPKsUDt"
      },
      "source": [
        "**Interpretation 5**: Interpret the results. \n",
        "- Did the model manage to guess the correct answers to the analogies (taking the first word in the output to be the model's \"guess\")? \n",
        "- Are the correct answers present in the top K words? \n",
        "- Do you see any patterns in the cases when the model didn't solve the task correctly? In other words, when the model's guess was wrong, can you suggest why the model guessed what it guessed?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc7_PHbXRqyZ"
      },
      "source": [
        "1) The model managed to gues the correct analogies in most cases as \"scabbers\" and \"ron\" have the same relation as \"hedwig\" and \"harry\" as they both have the realtion of being the pet of one another. The same is true for: \"avada kadavra\" and \"expecto patronum\", \"dungeon slytherin\" and \"tower gryffindor\", \"harry seeker\" and \"ron captain\". All of these anagolies are correct in the sense that they represent the same relation between the words. There are however some cases that are not correct, for example, the relation of \"padma\" and \"parvati\" is not the same as the relation of \"fred\" and \"weasley\" as \"padma\" and \"parvati\" are sisters, but \"weasley\" is the last name of \"fred\".\n",
        "\n",
        "2) for most cases, the correct answers are present in the top K words. For instance, in avada is to kedavra as expecto is to 'patronum'. patronum is the best hit and also the correct answer. Also, the dungeon to slytherin as tower is to gryffindor is correct and harry is to seeker as ron is to captain is correct.\n",
        "\n",
        "However, a few instances are not correct. Padma is to pervati as fred is to 'george', but george is not part of the list. Also, snape is to potions as trelawney is to 'divinations', but divinations in not present in the top K words.\n",
        "\n",
        "So, in most cases the correct answer is present but not in all.\n",
        "\n",
        "3) In cases where the first two words co-occur very frequently (like 'padma' and 'parvati') the difference in context is so small that there isn't a clear direction given to the third word, so 'fred' becomes 'weasly' (after filtering out the third word, so 'fred', which it would've been because the change is so small). 'Weasly' simply co-occurs much more frequently in the same context with 'fred' than 'george' does, and the algorithm does't really search for anything else than simply sharing the same context.\n",
        "\n",
        "Similarly, when 'Durmstrang', 'viktor' and 'beauxbatons' is the input the output is 'viktor'. This is again due to the algorithm looking purely for (somewhat) shared contexts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8DhKE1zsUDt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUKW52L5sUDt"
      },
      "source": [
        "### Optional\n",
        "If you are done, you can continue experimenting in order to understand the system's behaviour better. For example: how does training and hyperparameter choice affect the model's performance?\n",
        "Repeat the training using your own hyperparameters (vector space dimensionality, optimizer parameters, the number of training epochs, etc.). \n",
        "\n",
        "During the training loop, print the qualitative benchmarks every few epochs. Do they keep improving? Is there any disadvantage to exhaustively training until convergence?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXBT9MPbsUDt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}